{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribed into Korean, Thank you all\n",
    "\n",
    "이 노트북은 \"[Enefit PEBOP: EDA (Plotly) and Modelling](https://www.kaggle.com/code/siddhvr/enefit-pebop-eda-plotly-and-modelling?scriptVersionId=158742203)\" 노트북의 약간 개선된 버전, <br>\n",
    "즉 공개 LB에서 가장 높은 점수를 받은 3번째 버전입니다. \n",
    "\n",
    "변경 사항 목록:\n",
    "- 학습된 모델은 이제 노트북이 아닌 외부  [dataset](https://www.kaggle.com/datasets/kononenko/v2-enefit-pebop-eda-plotly-and-modelling/data)에서 로드됩니다. 이로써 새 노트북 버전이 생성될 경우의 문제가 해결되었습니다;\n",
    "- 앙상블 가중치가 LB 점수를 최대화하도록 조정되었습니다. \n",
    "\n",
    "이 노트북이 유용하다고 생각되시면 @siddhvr의 원본 작업을 참고해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polars\n",
      "  Downloading polars-0.20.31-cp38-abi3-win_amd64.whl.metadata (14 kB)\n",
      "Downloading polars-0.20.31-cp38-abi3-win_amd64.whl (28.8 MB)\n",
      "   ---------------------------------------- 0.0/28.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/28.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/28.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/28.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/28.8 MB 3.4 MB/s eta 0:00:09\n",
      "   ---------------------------------------- 0.2/28.8 MB 3.6 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.6/28.8 MB 6.1 MB/s eta 0:00:05\n",
      "    --------------------------------------- 0.6/28.8 MB 6.1 MB/s eta 0:00:05\n",
      "    --------------------------------------- 0.6/28.8 MB 6.1 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.9/28.8 MB 3.5 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 1.4/28.8 MB 4.7 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.4/28.8 MB 4.7 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.4/28.8 MB 4.7 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 1.7/28.8 MB 3.9 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 2.0/28.8 MB 4.0 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 2.2/28.8 MB 4.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 2.2/28.8 MB 4.4 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 2.3/28.8 MB 3.6 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 2.7/28.8 MB 4.1 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 3.1/28.8 MB 4.4 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 3.1/28.8 MB 4.4 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 3.1/28.8 MB 4.4 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 3.4/28.8 MB 4.0 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 4.0/28.8 MB 4.5 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 4.0/28.8 MB 4.5 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 4.0/28.8 MB 4.5 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 4.2/28.8 MB 4.0 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.4/28.8 MB 4.0 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.4/28.8 MB 4.0 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.4/28.8 MB 4.0 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.5/28.8 MB 3.6 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.7/28.8 MB 3.7 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 5.0/28.8 MB 3.8 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 5.4/28.8 MB 3.9 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 5.7/28.8 MB 4.0 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 6.1/28.8 MB 4.2 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 6.4/28.8 MB 4.2 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 6.7/28.8 MB 4.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 7.1/28.8 MB 4.4 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 7.5/28.8 MB 4.5 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 8.0/28.8 MB 4.7 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 8.3/28.8 MB 4.7 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 8.9/28.8 MB 4.9 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 9.2/28.8 MB 5.0 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 9.7/28.8 MB 5.2 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 10.0/28.8 MB 5.2 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 10.4/28.8 MB 5.3 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 10.9/28.8 MB 5.6 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 11.5/28.8 MB 5.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 12.1/28.8 MB 6.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 12.5/28.8 MB 6.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 13.0/28.8 MB 6.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 13.6/28.8 MB 7.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 14.3/28.8 MB 8.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 15.1/28.8 MB 9.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 15.9/28.8 MB 10.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 16.8/28.8 MB 11.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 17.5/28.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 18.3/28.8 MB 12.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 19.1/28.8 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 20.0/28.8 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 20.8/28.8 MB 15.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 21.3/28.8 MB 15.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 21.9/28.8 MB 15.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 22.6/28.8 MB 15.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 23.4/28.8 MB 16.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 24.2/28.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 25.1/28.8 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 26.1/28.8 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 26.7/28.8 MB 16.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 27.5/28.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  28.2/28.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  28.8/28.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  28.8/28.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  28.8/28.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  28.8/28.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.8/28.8 MB 12.8 MB/s eta 0:00:00\n",
      "Installing collected packages: polars\n",
      "Successfully installed polars-0.20.31\n"
     ]
    }
   ],
   "source": [
    "! pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.3.0-py3-none-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\hanyk\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\hanyk\\anaconda3\\lib\\site-packages (from lightgbm) (1.11.4)\n",
      "Downloading lightgbm-4.3.0-py3-none-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.3 MB 383.3 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.3/1.3 MB 983.0 kB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.3/1.3 MB 983.0 kB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.3/1.3 MB 983.0 kB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.4/1.3 MB 919.0 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 0.8/1.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.0/1.3 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.0/1.3 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.1/1.3 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.2/1.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.3.0\n"
     ]
    }
   ],
   "source": [
    "! pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting holidays\n",
      "  Downloading holidays-0.50-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\hanyk\\anaconda3\\lib\\site-packages (from holidays) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hanyk\\anaconda3\\lib\\site-packages (from python-dateutil->holidays) (1.16.0)\n",
      "Downloading holidays-0.50-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.1/1.0 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.6/1.0 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 7.3 MB/s eta 0:00:00\n",
      "Installing collected packages: holidays\n",
      "Successfully installed holidays-0.50\n"
     ]
    }
   ],
   "source": [
    "! pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import joblib\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "import lightgbm as lgb\n",
    "from joblib import load\n",
    "\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 주 GPU 할당량을 모두 소진했기 때문에 이 노트북의 보너스 버전입니다 🙃. \n",
    "여기서는 학습된 두 모델의 예측을 앙상블해 보았습니다. 하지만 문제는 두 모델이 서로 다른 수의 특징을 가진 데이터 세트로 학습되었다는 것입니다. 이 차이가 최종 예측에 미치는 영향을 살펴보려고 노력했습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 데이터 세트로 게시한 훈련된 모델을 사용하겠습니다. 이 모델은 공개 테스트 세트(리더보드)에서 65.51의 독립적인 MAE를 기록했으며, 다른 모델은 이 노트북 [see here](https://www.kaggle.com/code/siddhvr/enefit-pebop-eda-plotly-and-modelling) 의 버전 참조를 통해 학습한 모델이 될 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : 모든 데이터 파일에 액세스하고 처리하는 클래스를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStorage:\n",
    "    #root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
    "    root = \"predict-energy-behavior-of-prosumers/\"\n",
    "\n",
    "    data_cols = [\n",
    "        \"target\",\n",
    "        \"county\",\n",
    "        \"is_business\",\n",
    "        \"product_type\",\n",
    "        \"is_consumption\",\n",
    "        \"datetime\",\n",
    "        \"row_id\",\n",
    "    ]\n",
    "    client_cols = [\n",
    "        \"product_type\",\n",
    "        \"county\",\n",
    "        \"eic_count\",\n",
    "        \"installed_capacity\",\n",
    "        \"is_business\",\n",
    "        \"date\",\n",
    "    ]\n",
    "    gas_prices_cols = [\"forecast_date\", \"lowest_price_per_mwh\", \"highest_price_per_mwh\"]\n",
    "    electricity_prices_cols = [\"forecast_date\", \"euros_per_mwh\"]\n",
    "    forecast_weather_cols = [\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"hours_ahead\",\n",
    "        \"temperature\",\n",
    "        \"dewpoint\",\n",
    "        \"cloudcover_high\",\n",
    "        \"cloudcover_low\",\n",
    "        \"cloudcover_mid\",\n",
    "        \"cloudcover_total\",\n",
    "        \"10_metre_u_wind_component\",\n",
    "        \"10_metre_v_wind_component\",\n",
    "        \"forecast_datetime\",\n",
    "        \"direct_solar_radiation\",\n",
    "        \"surface_solar_radiation_downwards\",\n",
    "        \"snowfall\",\n",
    "        \"total_precipitation\",\n",
    "    ]\n",
    "    historical_weather_cols = [\n",
    "        \"datetime\",\n",
    "        \"temperature\",\n",
    "        \"dewpoint\",\n",
    "        \"rain\",\n",
    "        \"snowfall\",\n",
    "        \"surface_pressure\",\n",
    "        \"cloudcover_total\",\n",
    "        \"cloudcover_low\",\n",
    "        \"cloudcover_mid\",\n",
    "        \"cloudcover_high\",\n",
    "        \"windspeed_10m\",\n",
    "        \"winddirection_10m\",\n",
    "        \"shortwave_radiation\",\n",
    "        \"direct_solar_radiation\",\n",
    "        \"diffuse_radiation\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "    ]\n",
    "    location_cols = [\"longitude\", \"latitude\", \"county\"]\n",
    "    target_cols = [\n",
    "        \"target\",\n",
    "        \"county\",\n",
    "        \"is_business\",\n",
    "        \"product_type\",\n",
    "        \"is_consumption\",\n",
    "        \"datetime\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.df_data = pl.read_csv(\n",
    "            os.path.join(self.root, \"train.csv\"),\n",
    "            columns=self.data_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_client = pl.read_csv(\n",
    "            os.path.join(self.root, \"client.csv\"),\n",
    "            columns=self.client_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_gas_prices = pl.read_csv(\n",
    "            os.path.join(self.root, \"gas_prices.csv\"),\n",
    "            columns=self.gas_prices_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_electricity_prices = pl.read_csv(\n",
    "            os.path.join(self.root, \"electricity_prices.csv\"),\n",
    "            columns=self.electricity_prices_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_forecast_weather = pl.read_csv(\n",
    "            os.path.join(self.root, \"forecast_weather.csv\"),\n",
    "            columns=self.forecast_weather_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_historical_weather = pl.read_csv(\n",
    "            os.path.join(self.root, \"historical_weather.csv\"),\n",
    "            columns=self.historical_weather_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_weather_station_to_county_mapping = pl.read_csv(\n",
    "            os.path.join(self.root, \"weather_station_to_county_mapping.csv\"),\n",
    "            columns=self.location_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_data = self.df_data.filter(\n",
    "            pl.col(\"datetime\") >= pd.to_datetime(\"2022-01-01\")\n",
    "        )\n",
    "        self.df_target = self.df_data.select(self.target_cols)\n",
    "\n",
    "        self.schema_data = self.df_data.schema\n",
    "        self.schema_client = self.df_client.schema\n",
    "        self.schema_gas_prices = self.df_gas_prices.schema\n",
    "        self.schema_electricity_prices = self.df_electricity_prices.schema\n",
    "        self.schema_forecast_weather = self.df_forecast_weather.schema\n",
    "        self.schema_historical_weather = self.df_historical_weather.schema\n",
    "        self.schema_target = self.df_target.schema\n",
    "\n",
    "        self.df_weather_station_to_county_mapping = (\n",
    "            self.df_weather_station_to_county_mapping.with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def update_with_new_data(\n",
    "        self,\n",
    "        df_new_client,\n",
    "        df_new_gas_prices,\n",
    "        df_new_electricity_prices,\n",
    "        df_new_forecast_weather,\n",
    "        df_new_historical_weather,\n",
    "        df_new_target,\n",
    "    ):\n",
    "        df_new_client = pl.from_pandas(\n",
    "            df_new_client[self.client_cols], schema_overrides=self.schema_client\n",
    "        )\n",
    "        df_new_gas_prices = pl.from_pandas(\n",
    "            df_new_gas_prices[self.gas_prices_cols],\n",
    "            schema_overrides=self.schema_gas_prices,\n",
    "        )\n",
    "        df_new_electricity_prices = pl.from_pandas(\n",
    "            df_new_electricity_prices[self.electricity_prices_cols],\n",
    "            schema_overrides=self.schema_electricity_prices,\n",
    "        )\n",
    "        df_new_forecast_weather = pl.from_pandas(\n",
    "            df_new_forecast_weather[self.forecast_weather_cols],\n",
    "            schema_overrides=self.schema_forecast_weather,\n",
    "        )\n",
    "        df_new_historical_weather = pl.from_pandas(\n",
    "            df_new_historical_weather[self.historical_weather_cols],\n",
    "            schema_overrides=self.schema_historical_weather,\n",
    "        )\n",
    "        df_new_target = pl.from_pandas(\n",
    "            df_new_target[self.target_cols], schema_overrides=self.schema_target\n",
    "        )\n",
    "\n",
    "        self.df_client = pl.concat([self.df_client, df_new_client]).unique(\n",
    "            [\"date\", \"county\", \"is_business\", \"product_type\"]\n",
    "        )\n",
    "        self.df_gas_prices = pl.concat([self.df_gas_prices, df_new_gas_prices]).unique(\n",
    "            [\"forecast_date\"]\n",
    "        )\n",
    "        self.df_electricity_prices = pl.concat(\n",
    "            [self.df_electricity_prices, df_new_electricity_prices]\n",
    "        ).unique([\"forecast_date\"])\n",
    "        self.df_forecast_weather = pl.concat(\n",
    "            [self.df_forecast_weather, df_new_forecast_weather]\n",
    "        ).unique([\"forecast_datetime\", \"latitude\", \"longitude\", \"hours_ahead\"])\n",
    "        self.df_historical_weather = pl.concat(\n",
    "            [self.df_historical_weather, df_new_historical_weather]\n",
    "        ).unique([\"datetime\", \"latitude\", \"longitude\"])\n",
    "        self.df_target = pl.concat([self.df_target, df_new_target]).unique(\n",
    "            [\"datetime\", \"county\", \"is_business\", \"product_type\", \"is_consumption\"]\n",
    "        )\n",
    "\n",
    "    def preprocess_test(self, df_test):\n",
    "        df_test = df_test.rename(columns={\"prediction_datetime\": \"datetime\"})\n",
    "        df_test = pl.from_pandas(\n",
    "            df_test[self.data_cols[1:]], schema_overrides=self.schema_data\n",
    "        )\n",
    "        return df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: 피처 엔지니어링 클래스 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이해하기 쉽고 명확하게 설명하기 위해 두 모델에 대해 서로 다른 두 가지 기능 엔지니어링 클래스를 사용하겠습니다. 그러나 대부분의 함수는 다른 곳에서 약간의 변경만 있을 뿐 동일하므로 두 클래스를 병합하여 하나의 클래스로 만들 수도 있습니다. 하지만 이 경우 두 모델 모두에 대해 서로 다른 두 개의 \"generate_features\" 함수를 사용해야 한다는 점을 잊지 마세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.estonian_holidays = list(\n",
    "            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n",
    "        )\n",
    "\n",
    "    def _general_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                pl.col(\"datetime\").dt.day().alias(\"day\"),pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "                pl.col(\"datetime\").dt.month().alias(\"month\"),pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "            )\n",
    "            .with_columns(pl.concat_str(\"county\",\"is_business\",\"product_type\",\"is_consumption\",separator=\"_\",).alias(\"segment\"),)\n",
    "            .with_columns(\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),(np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),(np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _client_features(self, df_features):\n",
    "        df_client = self.data.df_client\n",
    "\n",
    "        df_features = df_features.join(\n",
    "            df_client.with_columns((pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
    "            ),on=[\"county\", \"is_business\", \"product_type\", \"date\"],how=\"left\",\n",
    "        )\n",
    "        return df_features\n",
    "    \n",
    "    def is_country_holiday(self, row):\n",
    "        return (\n",
    "            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n",
    "            in self.estonian_holidays\n",
    "        )\n",
    "\n",
    "    def _holidays_features(self, df_features):\n",
    "        df_features = df_features.with_columns(\n",
    "            pl.struct([\"year\", \"month\", \"day\"])\n",
    "            .apply(self.is_country_holiday)\n",
    "            .alias(\"is_country_holiday\")\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _forecast_weather_features(self, df_features):\n",
    "        df_forecast_weather = self.data.df_forecast_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_forecast_weather = (\n",
    "            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n",
    "            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n",
    "#             .drop(\"hours_ahead\")\n",
    "            .with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),)\n",
    "            .join(df_weather_station_to_county_mapping,how=\"left\",on=[\"longitude\", \"latitude\"],).drop(\"longitude\", \"latitude\"))\n",
    "\n",
    "        df_forecast_weather_date = (df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\"))\n",
    "\n",
    "        df_forecast_weather_local = (df_forecast_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean())\n",
    "\n",
    "        for hours_lag in [0, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),on=\"datetime\",how=\"left\",suffix=f\"_forecast_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),on=[\"county\", \"datetime\"],how=\"left\",suffix=f\"_forecast_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _historical_weather_features(self, df_features):\n",
    "        df_historical_weather = self.data.df_historical_weather\n",
    "        df_weather_station_to_county_mapping = (self.data.df_weather_station_to_county_mapping)\n",
    "\n",
    "        df_historical_weather = (\n",
    "            df_historical_weather.with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            ).join(df_weather_station_to_county_mapping,how=\"left\",on=[\"longitude\", \"latitude\"],).drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_date = (df_historical_weather.group_by(\"datetime\").mean().drop(\"county\"))\n",
    "\n",
    "        df_historical_weather_local = (\n",
    "            df_historical_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [2 * 24, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),on=\"datetime\",how=\"left\",suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),on=[\"county\", \"datetime\"],how=\"left\",suffix=f\"_historical_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [1 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n",
    "                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),).filter(pl.col(\"hour\") <= 10).drop(\"hour\"),on=\"datetime\",how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _target_features(self, df_features):\n",
    "        df_target = self.data.df_target\n",
    "\n",
    "        df_target_all_type_sum = (df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\"))\n",
    "\n",
    "        df_target_all_county_type_sum = (df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\", \"county\"))\n",
    "        \n",
    "        hours_list=[i*24 for i in range(2,15)]\n",
    "\n",
    "        for hours_lag in hours_list:\n",
    "            df_features = df_features.join(\n",
    "                df_target.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n",
    "                on=[\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],how=\"left\",\n",
    "            )\n",
    "\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_county_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"is_business\", \"is_consumption\", \"datetime\"],how=\"left\",\n",
    "                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        cols_for_stats = [f\"target_{hours_lag}h\" for hours_lag in hours_list[:4]]\n",
    "        \n",
    "        df_features = df_features.with_columns(\n",
    "            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n",
    "            df_features.select(cols_for_stats).transpose().std().transpose().to_series().alias(f\"target_std\"),\n",
    "            )\n",
    "\n",
    "        for target_prefix, lag_nominator, lag_denomonator in [\n",
    "            (\"target\", 24 * 7, 24 * 14),(\"target\", 24 * 2, 24 * 9),(\"target\", 24 * 3, 24 * 10),(\"target\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 2, 24 * 3),(\"target_all_type_sum\", 24 * 7, 24 * 14),\n",
    "            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),(\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n",
    "        ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (pl.col(f\"{target_prefix}_{lag_nominator}h\")/ (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n",
    "                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _reduce_memory_usage(self, df_features):\n",
    "        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n",
    "        return df_features\n",
    "\n",
    "    def _drop_columns(self, df_features):\n",
    "        df_features = df_features.drop(\"datetime\", \"hour\", \"dayofyear\")\n",
    "        return df_features\n",
    "\n",
    "    def _to_pandas(self, df_features, y):\n",
    "        cat_cols = [\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"segment\",]\n",
    "\n",
    "        if y is not None:\n",
    "            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n",
    "        else:\n",
    "            df_features = df_features.to_pandas()\n",
    "\n",
    "        df_features = df_features.set_index(\"row_id\")\n",
    "        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n",
    "\n",
    "        return df_features\n",
    "    \n",
    "    # added some new features here\n",
    "    def _additional_features(self,df):\n",
    "        for col in [\n",
    "                    'temperature', \n",
    "                    'dewpoint', \n",
    "                    '10_metre_u_wind_component', \n",
    "                    '10_metre_v_wind_component', \n",
    "            ]:\n",
    "            for window in [1]:\n",
    "                df[f\"{col}_diff_{window}\"] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])[col].diff(window)\n",
    "        return df\n",
    "    \n",
    "    def _log_outliers(self,df):\n",
    "        l1=['installed_capacity', 'target_mean', 'target_std']\n",
    "        for i in l1:\n",
    "            df = df.with_columns([(f\"log_{i}\", pl.when(df[i] != 0).then(np.log(pl.col(i))).otherwise(0))])\n",
    "        return df\n",
    "        \n",
    "\n",
    "    def generate_features(self, df_prediction_items,isTrain):\n",
    "        if \"target\" in df_prediction_items.columns:\n",
    "            df_prediction_items, y = (\n",
    "                df_prediction_items.drop(\"target\"),\n",
    "                df_prediction_items.select(\"target\"),\n",
    "            )\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        df_features = df_prediction_items.with_columns(\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "        )\n",
    "\n",
    "        for add_features in [\n",
    "            self._general_features,self._client_features,self._forecast_weather_features,\n",
    "            self._historical_weather_features,self._target_features,self._holidays_features,\n",
    "            self._log_outliers,self._reduce_memory_usage,self._drop_columns,]:\n",
    "            df_features = add_features(df_features)\n",
    "\n",
    "        df_features = self._to_pandas(df_features, y)\n",
    "        df_features = self._additional_features(df_features)\n",
    "\n",
    "        return df_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class FeaturesGenerator:\n",
    "    def __init__(self, data_storage):\n",
    "        self.data_storage = data_storage\n",
    "        self.estonian_holidays = list(\n",
    "            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n",
    "        )\n",
    "\n",
    "    def _add_general_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n",
    "                pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
    "                pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "                pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
    "                pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.concat_str(\n",
    "                    \"county\",\n",
    "                    \"is_business\",\n",
    "                    \"product_type\",\n",
    "                    \"is_consumption\",\n",
    "                    separator=\"_\",\n",
    "                ).alias(\"segment\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _add_client_features(self, df_features):\n",
    "        df_client = self.data_storage.df_client\n",
    "\n",
    "        df_features = df_features.join(\n",
    "            df_client.with_columns(\n",
    "                (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
    "            ),\n",
    "            on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        return df_features\n",
    "    \n",
    "    def is_country_holiday(self, row):\n",
    "        return (\n",
    "            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n",
    "            in self.estonian_holidays\n",
    "        )\n",
    "\n",
    "    def _add_holidays_features(self, df_features):\n",
    "        df_features = df_features.with_columns(\n",
    "            pl.struct([\"year\", \"month\", \"day\"])\n",
    "            .apply(self.is_country_holiday)\n",
    "            .alias(\"is_country_holiday\")\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _add_forecast_weather_features(self, df_features):\n",
    "        df_forecast_weather = self.data_storage.df_forecast_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_forecast_weather = (\n",
    "            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n",
    "            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n",
    "            .drop(\"hours_ahead\")\n",
    "            .with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_forecast_weather_date = (\n",
    "            df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\")\n",
    "        )\n",
    "\n",
    "        df_forecast_weather_local = (\n",
    "            df_forecast_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [0, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _add_historical_weather_features(self, df_features):\n",
    "        df_historical_weather = self.data_storage.df_historical_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_historical_weather = (\n",
    "            df_historical_weather.with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_date = (\n",
    "            df_historical_weather.group_by(\"datetime\").mean().drop(\"county\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_local = (\n",
    "            df_historical_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [2 * 24, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [1 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n",
    "                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                )\n",
    "                .filter(pl.col(\"hour\") <= 10)\n",
    "                .drop(\"hour\"),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _add_target_features(self, df_features):\n",
    "        df_target = self.data_storage.df_target\n",
    "\n",
    "        df_target_all_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\")\n",
    "        )\n",
    "\n",
    "        df_target_all_county_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\", \"county\")\n",
    "        )\n",
    "\n",
    "        for hours_lag in [2 * 24,3 * 24,4 * 24,5 * 24,6 * 24,7 * 24,8 * 24,9 * 24,10 * 24,11 * 24,12 * 24,13 * 24,14 * 24,]:\n",
    "            df_features = df_features.join(\n",
    "                df_target.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n",
    "                on=[\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_county_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        cols_for_stats = [\n",
    "            f\"target_{hours_lag}h\" for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24]\n",
    "        ]\n",
    "        df_features = df_features.with_columns(\n",
    "            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n",
    "            df_features.select(cols_for_stats)\n",
    "            .transpose()\n",
    "            .std()\n",
    "            .transpose()\n",
    "            .to_series()\n",
    "            .alias(f\"target_std\"),\n",
    "        )\n",
    "\n",
    "        for target_prefix, lag_nominator, lag_denomonator in [\n",
    "            (\"target\", 24 * 7, 24 * 14),\n",
    "            (\"target\", 24 * 2, 24 * 9),\n",
    "            (\"target\", 24 * 3, 24 * 10),\n",
    "            (\"target\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 7, 24 * 14),\n",
    "            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n",
    "        ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{target_prefix}_{lag_nominator}h\")\n",
    "                    / (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n",
    "                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _reduce_memory_usage(self, df_features):\n",
    "        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n",
    "        return df_features\n",
    "\n",
    "    def _drop_columns(self, df_features):\n",
    "        df_features = df_features.drop(\n",
    "            \"date\", \"datetime\", \"hour\", \"dayofyear\"\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _to_pandas(self, df_features, y):\n",
    "        cat_cols = [\n",
    "            \"county\",\n",
    "            \"is_business\",\n",
    "            \"product_type\",\n",
    "            \"is_consumption\",\n",
    "            \"segment\",\n",
    "        ]\n",
    "\n",
    "        if y is not None:\n",
    "            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n",
    "        else:\n",
    "            df_features = df_features.to_pandas()\n",
    "\n",
    "        df_features = df_features.set_index(\"row_id\")\n",
    "        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n",
    "\n",
    "        return df_features\n",
    "    \n",
    "    def generate_features(self, df_prediction_items):\n",
    "        if \"target\" in df_prediction_items.columns:\n",
    "            df_prediction_items, y = (\n",
    "                df_prediction_items.drop(\"target\"),\n",
    "                df_prediction_items.select(\"target\"),\n",
    "            )\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        df_features = df_prediction_items.with_columns(\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "        )\n",
    "\n",
    "        for add_features in [\n",
    "            self._add_general_features,\n",
    "            self._add_client_features,\n",
    "            self._add_forecast_weather_features,\n",
    "            self._add_historical_weather_features,\n",
    "            self._add_target_features,\n",
    "            self._add_holidays_features,\n",
    "            self._reduce_memory_usage,\n",
    "            self._drop_columns,\n",
    "        ]:\n",
    "            df_features = add_features(df_features)\n",
    "\n",
    "        df_features = self._to_pandas(df_features, y)\n",
    "\n",
    "        return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 초기화 Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_storage = DataStorage()\n",
    "features_generator = FeaturesGenerator(data_storage=data_storage)\n",
    "feat_gen = FeatureEngineer(data=data_storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 피쳐 생성 Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "트레이닝 데이터 세트를 생성할 필요는 없지만, (여기서는 모델 트레이닝을 수행하지 않기 때문에) 여기서 사용하는 두 데이터 세트 간의 기능 차이를 보여주기 위해서만 생성했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features = features_generator.generate_features(data_storage.df_data)\n",
    "df_train_features = df_train_features[df_train_features['target'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unexpected value while building Series of type String; found value of type Object(\"object\", None): .when(Series[installed_capacity]).then(col(\"installed_capacity\").python_udf()).otherwise(dyn int: 0)\n\nHint: Try setting `strict=False` to allow passing data with mixed types.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_train \u001b[38;5;241m=\u001b[39m feat_gen\u001b[38;5;241m.\u001b[39mgenerate_features(data_storage\u001b[38;5;241m.\u001b[39mdf_data,\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m df_train \u001b[38;5;241m=\u001b[39m df_train[df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotnull()]\n",
      "Cell \u001b[1;32mIn[33], line 220\u001b[0m, in \u001b[0;36mFeatureEngineer.generate_features\u001b[1;34m(self, df_prediction_items, isTrain)\u001b[0m\n\u001b[0;32m    212\u001b[0m df_features \u001b[38;5;241m=\u001b[39m df_prediction_items\u001b[38;5;241m.\u001b[39mwith_columns(\n\u001b[0;32m    213\u001b[0m     pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(pl\u001b[38;5;241m.\u001b[39mDate)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    214\u001b[0m )\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m add_features \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_general_features,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_features,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forecast_weather_features,\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_historical_weather_features,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target_features,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_holidays_features,\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_outliers,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce_memory_usage,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drop_columns,]:\n\u001b[1;32m--> 220\u001b[0m     df_features \u001b[38;5;241m=\u001b[39m add_features(df_features)\n\u001b[0;32m    222\u001b[0m df_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_pandas(df_features, y)\n\u001b[0;32m    223\u001b[0m df_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_additional_features(df_features)\n",
      "Cell \u001b[1;32mIn[33], line 199\u001b[0m, in \u001b[0;36mFeatureEngineer._log_outliers\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m    197\u001b[0m l1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstalled_capacity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_mean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_std\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m l1:\n\u001b[1;32m--> 199\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwith_columns([(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, pl\u001b[38;5;241m.\u001b[39mwhen(df[i] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mthen(np\u001b[38;5;241m.\u001b[39mlog(pl\u001b[38;5;241m.\u001b[39mcol(i)))\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;241m0\u001b[39m))])\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\polars\\dataframe\\frame.py:8634\u001b[0m, in \u001b[0;36mDataFrame.with_columns\u001b[1;34m(self, *exprs, **named_exprs)\u001b[0m\n\u001b[0;32m   8488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_columns\u001b[39m(\n\u001b[0;32m   8489\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   8490\u001b[0m     \u001b[38;5;241m*\u001b[39mexprs: IntoExpr \u001b[38;5;241m|\u001b[39m Iterable[IntoExpr],\n\u001b[0;32m   8491\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnamed_exprs: IntoExpr,\n\u001b[0;32m   8492\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   8493\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   8494\u001b[0m \u001b[38;5;124;03m    Add columns to this DataFrame.\u001b[39;00m\n\u001b[0;32m   8495\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   8632\u001b[0m \u001b[38;5;124;03m    └─────┴──────┴─────────────┘\u001b[39;00m\n\u001b[0;32m   8633\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 8634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy()\u001b[38;5;241m.\u001b[39mwith_columns(\u001b[38;5;241m*\u001b[39mexprs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnamed_exprs)\u001b[38;5;241m.\u001b[39mcollect(_eager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\polars\\lazyframe\\frame.py:4347\u001b[0m, in \u001b[0;36mLazyFrame.with_columns\u001b[1;34m(self, *exprs, **named_exprs)\u001b[0m\n\u001b[0;32m   4204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4205\u001b[0m \u001b[38;5;124;03mAdd columns to this LazyFrame.\u001b[39;00m\n\u001b[0;32m   4206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4343\u001b[0m \u001b[38;5;124;03m└─────┴──────┴─────────────┘\u001b[39;00m\n\u001b[0;32m   4344\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4345\u001b[0m structify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOLARS_AUTO_STRUCTIFY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m-> 4347\u001b[0m pyexprs \u001b[38;5;241m=\u001b[39m parse_as_list_of_expressions(\n\u001b[0;32m   4348\u001b[0m     \u001b[38;5;241m*\u001b[39mexprs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnamed_exprs, __structify\u001b[38;5;241m=\u001b[39mstructify\n\u001b[0;32m   4349\u001b[0m )\n\u001b[0;32m   4350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pyldf(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ldf\u001b[38;5;241m.\u001b[39mwith_columns(pyexprs))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\polars\\_utils\\parse_expr_input.py:42\u001b[0m, in \u001b[0;36mparse_as_list_of_expressions\u001b[1;34m(__structify, *inputs, **named_inputs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_as_list_of_expressions\u001b[39m(\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;241m*\u001b[39minputs: IntoExpr \u001b[38;5;241m|\u001b[39m Iterable[IntoExpr],\n\u001b[0;32m     22\u001b[0m     __structify: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnamed_inputs: IntoExpr,\n\u001b[0;32m     24\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[PyExpr]:\n\u001b[0;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m    Parse multiple inputs into a list of expressions.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    list of PyExpr\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     exprs \u001b[38;5;241m=\u001b[39m _parse_positional_inputs(inputs, structify\u001b[38;5;241m=\u001b[39m__structify)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m named_inputs:\n\u001b[0;32m     44\u001b[0m         named_exprs \u001b[38;5;241m=\u001b[39m _parse_named_inputs(named_inputs, structify\u001b[38;5;241m=\u001b[39m__structify)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\polars\\_utils\\parse_expr_input.py:56\u001b[0m, in \u001b[0;36m_parse_positional_inputs\u001b[1;34m(inputs, structify)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse_positional_inputs\u001b[39m(\n\u001b[0;32m     51\u001b[0m     inputs: \u001b[38;5;28mtuple\u001b[39m[IntoExpr, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Iterable[IntoExpr]],\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m     53\u001b[0m     structify: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     54\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[PyExpr]:\n\u001b[0;32m     55\u001b[0m     inputs_iter \u001b[38;5;241m=\u001b[39m _parse_inputs_as_iterable(inputs)\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [parse_as_expression(e, structify\u001b[38;5;241m=\u001b[39mstructify) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m inputs_iter]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\polars\\_utils\\parse_expr_input.py:56\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse_positional_inputs\u001b[39m(\n\u001b[0;32m     51\u001b[0m     inputs: \u001b[38;5;28mtuple\u001b[39m[IntoExpr, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Iterable[IntoExpr]],\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m     53\u001b[0m     structify: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     54\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[PyExpr]:\n\u001b[0;32m     55\u001b[0m     inputs_iter \u001b[38;5;241m=\u001b[39m _parse_inputs_as_iterable(inputs)\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [parse_as_expression(e, structify\u001b[38;5;241m=\u001b[39mstructify) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m inputs_iter]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\polars\\_utils\\parse_expr_input.py:125\u001b[0m, in \u001b[0;36mparse_as_expression\u001b[1;34m(input, str_as_lit, list_as_lit, structify, dtype)\u001b[0m\n\u001b[0;32m    123\u001b[0m     structify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     expr \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;28minput\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    126\u001b[0m     structify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m structify:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\polars\\functions\\lit.py:127\u001b[0m, in \u001b[0;36mlit\u001b[1;34m(value, dtype, allow_object)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lit(pl\u001b[38;5;241m.\u001b[39mSeries(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliteral\u001b[39m\u001b[38;5;124m\"\u001b[39m, value, dtype\u001b[38;5;241m=\u001b[39mdtype))\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lit(pl\u001b[38;5;241m.\u001b[39mSeries(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliteral\u001b[39m\u001b[38;5;124m\"\u001b[39m, [value], dtype\u001b[38;5;241m=\u001b[39mdtype))\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap_expr(plr\u001b[38;5;241m.\u001b[39mlit(value, allow_object))\u001b[38;5;241m.\u001b[39mcast(dtype)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\polars\\series\\series.py:316\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, name, values, dtype, strict, nan_to_null, dtype_if_empty)\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, Sequence):\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_s \u001b[38;5;241m=\u001b[39m sequence_to_pyseries(\n\u001b[0;32m    317\u001b[0m         name,\n\u001b[0;32m    318\u001b[0m         values,\n\u001b[0;32m    319\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    320\u001b[0m         strict\u001b[38;5;241m=\u001b[39mstrict,\n\u001b[0;32m    321\u001b[0m         nan_to_null\u001b[38;5;241m=\u001b[39mnan_to_null,\n\u001b[0;32m    322\u001b[0m     )\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_s \u001b[38;5;241m=\u001b[39m sequence_to_pyseries(name, [], dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\polars\\_utils\\construction\\series.py:269\u001b[0m, in \u001b[0;36msequence_to_pyseries\u001b[1;34m(name, values, dtype, strict, nan_to_null)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m python_dtype \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m PySeries\u001b[38;5;241m.\u001b[39mnew_from_any_values(name, values, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m Object:\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m PySeries\u001b[38;5;241m.\u001b[39mnew_object(name, values, strict)\n",
      "\u001b[1;31mTypeError\u001b[0m: unexpected value while building Series of type String; found value of type Object(\"object\", None): .when(Series[installed_capacity]).then(col(\"installed_capacity\").python_udf()).otherwise(dyn int: 0)\n\nHint: Try setting `strict=False` to allow passing data with mixed types."
     ]
    }
   ],
   "source": [
    "df_train = feat_gen.generate_features(data_storage.df_data,True)\n",
    "df_train = df_train[df_train['target'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for col in df_train.columns:\n",
    "    if (col not in df_train_features.columns):\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if('date' in df_train_features.columns):\n",
    "    df_train_features.drop(columns=['date'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 분석을 위해 Plotly를 사용할 것입니다. <br>\n",
    "Plotly에는 많은 데이터 포인트에서 이상값이나 이상 징후를 감지할 수 있는 **호버 도구 기능**이 있습니다. <br>\n",
    "결과 플롯은 매우 상호 작용이 가능하며 더 심층적인 분석을 위해 플롯의 특정 영역을 확대하고 집중할 수 있습니다. <br>\n",
    "그래프를 무한대로 사용자 지정할 수 있어 더욱 의미 있고 이해하기 쉬운 그래프를 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세그먼트별 에너지 소비량 segment-wise energy consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "segment_list = df_train.segment.unique()[:3]\n",
    "c=['orangered','blue','gold']\n",
    "i=0\n",
    "# Filter the dataset for the specific segment\n",
    "for seg in segment_list:\n",
    "    consumption_segment = df_train[df_train.segment == seg]\n",
    "\n",
    "    # Create a line plot using Plotly Express\n",
    "    fig = px.line(consumption_segment, x='date', y='target', \n",
    "              title=f'Target Over Time for Segment {seg}',\n",
    "              labels={'date': 'Date', 'target': 'Target'},\n",
    "              template='plotly_dark',line_shape='linear')\n",
    "    fig.update_traces(line=dict(color=c[i], width=1.5))\n",
    "\n",
    "    # Customize the x-axis date format and tick interval\n",
    "    fig.update_xaxes(type='date', tickformat='%Y-%m-%d', tickmode='linear', dtick=15)\n",
    "    i+=1\n",
    "    # Show the plot\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFT Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음에는 <a href=\"https://www.kaggle.com/chaozhuang\">CHAO ZHUANG</a>의 <a href=\"https://www.kaggle.com/code/chaozhuang/enefit-eda-w-fft-ssa-lgbm-voting-regressor\">amazing notebook</a>에서 공부했습니다. <br>\n",
    "그는 매우 심층적으로 분석하고 이동 중에도 모든 것을 설명해 주었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "segment_list = df_train.segment.unique()[:10]\n",
    "example_df = df_train[np.isin(df_train.segment, segment_list)]\n",
    "segments = example_df['segment'].unique()\n",
    "\n",
    "# Define periods in days and calculate corresponding frequencies\n",
    "periods = {'Annual': 365,'Semiannual': 365 / 2,'Quarterly': 365 / 4,'Monthly': 30,'Biweekly': 14,'Weekly': 7,'Semiweekly': 3.5}\n",
    "frequencies_for_periods = {k: 1 / v for k, v in periods.items()}\n",
    "\n",
    "# Initialize the figure for the spectra using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Convert the x-axis to a log scale\n",
    "fig.update_xaxes(type='log')\n",
    "\n",
    "# Plot the spectrum for each segment with offset\n",
    "for i, segment in enumerate(segments):\n",
    "    segment_data = example_df[example_df['segment'] == segment]['target']\n",
    "    fft_values = np.fft.fft(segment_data)\n",
    "    frequencies = np.fft.fftfreq(len(fft_values), d=1)\n",
    "    magnitudes = np.abs(fft_values)[frequencies > 0]\n",
    "    normalized_magnitudes = magnitudes / np.max(magnitudes)\n",
    "    positive_freqs = frequencies[frequencies > 0]\n",
    "\n",
    "    # Filter out frequencies corresponding to periods longer than 'Semiannual'\n",
    "    valid_freqs = positive_freqs[positive_freqs > frequencies_for_periods['Semiannual']]\n",
    "    valid_magnitudes = normalized_magnitudes[positive_freqs > frequencies_for_periods['Semiannual']]\n",
    "\n",
    "    # Offset each segment's spectrum for clarity\n",
    "    offset_magnitudes = valid_magnitudes + i\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=valid_freqs, y=offset_magnitudes, mode='lines', name=f'Segment {segment}'))\n",
    "\n",
    "# Customize the plot layout\n",
    "fig.update_layout(\n",
    "    title='Frequency Spectra of hourly target for Each Segment',\n",
    "    xaxis_title='Frequency',\n",
    "    yaxis_title='Normalized Magnitude + Offset',\n",
    "    xaxis=dict(tickvals=list(frequencies_for_periods.values()), ticktext=list(frequencies_for_periods.keys())),\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_plots_enefit(name):\n",
    "    # Initialize the figure for the spectrum using Plotly\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Convert the x-axis to a log scale\n",
    "    fig.update_xaxes(type='log')\n",
    "\n",
    "    # Plot the spectrum for the specified segment\n",
    "    segment_data = example_df[example_df['segment'] == '0_0_1_1'][name]\n",
    "    fft_values = np.fft.fft(segment_data)\n",
    "    frequencies = np.fft.fftfreq(len(fft_values), d=1)\n",
    "    magnitudes = np.abs(fft_values)[frequencies > 0]\n",
    "    positive_freqs = frequencies[frequencies > 0]\n",
    "\n",
    "    # Filter out frequencies corresponding to periods longer than 'Semiannual'\n",
    "    valid_freqs = positive_freqs[positive_freqs > frequencies_for_periods['Semiannual']]\n",
    "    valid_magnitudes = magnitudes[positive_freqs > frequencies_for_periods['Semiannual']]\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=valid_freqs, y=valid_magnitudes, mode='lines', name='0_0_1_1'))\n",
    "\n",
    "    #  Customize the plot layout\n",
    "    fig.update_layout(\n",
    "    title=f'{name} frequency spectrum',\n",
    "    xaxis_title='Frequency',\n",
    "    yaxis_title='Magnitude',\n",
    "    xaxis=dict(tickvals=list(frequencies_for_periods.values()), ticktext=list(frequencies_for_periods.keys())),\n",
    "    showlegend=True,\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T07:30:45.08493Z",
     "iopub.status.busy": "2024-01-17T07:30:45.084577Z",
     "iopub.status.idle": "2024-01-17T07:30:45.172689Z",
     "shell.execute_reply": "2024-01-17T07:30:45.171866Z",
     "shell.execute_reply.started": "2024-01-17T07:30:45.084891Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot_list=['temperature','direct_solar_radiation']\n",
    "for i in plot_list:\n",
    "    fft_plots_enefit(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T07:35:03.747897Z",
     "iopub.status.busy": "2024-01-17T07:35:03.746875Z",
     "iopub.status.idle": "2024-01-17T07:35:46.164811Z",
     "shell.execute_reply": "2024-01-17T07:35:46.164052Z",
     "shell.execute_reply.started": "2024-01-17T07:35:03.747843Z"
    }
   },
   "outputs": [],
   "source": [
    "c1 = load('/kaggle/input/enefit-trained-model/voting_regressor_consumption_model.joblib')\n",
    "p1 = load('/kaggle/input/enefit-trained-model/voting_regressor_production_model.joblib')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T07:35:46.166629Z",
     "iopub.status.busy": "2024-01-17T07:35:46.16631Z",
     "iopub.status.idle": "2024-01-17T07:36:33.989245Z",
     "shell.execute_reply": "2024-01-17T07:36:33.98833Z",
     "shell.execute_reply.started": "2024-01-17T07:35:46.166601Z"
    }
   },
   "outputs": [],
   "source": [
    "dc1 = load('/kaggle/input/v2-enefit-pebop-eda-plotly-and-modelling/voting_regressor_consumption_model.joblib')\n",
    "dp1 = load('/kaggle/input/v2-enefit-pebop-eda-plotly-and-modelling/voting_regressor_production_model.joblib')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 두 모델에 대해 별도의 예측 함수 선언하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T07:36:33.990486Z",
     "iopub.status.busy": "2024-01-17T07:36:33.990243Z",
     "iopub.status.idle": "2024-01-17T07:36:33.996182Z",
     "shell.execute_reply": "2024-01-17T07:36:33.995258Z",
     "shell.execute_reply.started": "2024-01-17T07:36:33.990464Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(df_features,model_consumption=c1,model_production=p1):\n",
    "    predictions = np.zeros(len(df_features))\n",
    "\n",
    "    mask = df_features[\"is_consumption\"] == 1\n",
    "    predictions[mask.values] = model_consumption.predict(\n",
    "            df_features[mask]\n",
    "    ).clip(0)\n",
    "\n",
    "    mask = df_features[\"is_consumption\"] == 0\n",
    "    predictions[mask.values] = model_production.predict(\n",
    "            df_features[mask]\n",
    "    ).clip(0)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T07:36:33.998168Z",
     "iopub.status.busy": "2024-01-17T07:36:33.997827Z",
     "iopub.status.idle": "2024-01-17T07:36:34.019417Z",
     "shell.execute_reply": "2024-01-17T07:36:34.018531Z",
     "shell.execute_reply.started": "2024-01-17T07:36:33.998145Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_model(df_features,hours_lag=48,model_consumption=dc1,model_production=dp1):\n",
    "    predictions = np.zeros(len(df_features))\n",
    "\n",
    "    mask = df_features[\"is_consumption\"] == 1\n",
    "    predictions[mask.values] = np.clip(\n",
    "        df_features[mask][f\"target_{hours_lag}h\"].fillna(0).values+ \n",
    "        model_consumption.predict(df_features[mask]),0,np.inf,\n",
    "        )\n",
    "\n",
    "    mask = df_features[\"is_consumption\"] == 0\n",
    "    predictions[mask.values] = np.clip(\n",
    "        df_features[mask][f\"target_{hours_lag}h\"].fillna(0).values+ \n",
    "        model_production.predict(df_features[mask]),0,np.inf,\n",
    "        )\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T07:38:38.935081Z",
     "iopub.status.busy": "2024-01-17T07:38:38.934152Z",
     "iopub.status.idle": "2024-01-17T07:38:38.943435Z",
     "shell.execute_reply": "2024-01-17T07:38:38.942651Z",
     "shell.execute_reply.started": "2024-01-17T07:38:38.935047Z"
    }
   },
   "outputs": [],
   "source": [
    "import enefit\n",
    "\n",
    "env = enefit.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T07:38:39.471463Z",
     "iopub.status.busy": "2024-01-17T07:38:39.470629Z",
     "iopub.status.idle": "2024-01-17T07:40:35.931735Z",
     "shell.execute_reply": "2024-01-17T07:40:35.930711Z",
     "shell.execute_reply.started": "2024-01-17T07:38:39.471433Z"
    }
   },
   "outputs": [],
   "source": [
    "for (\n",
    "    df_test, \n",
    "    df_new_target, \n",
    "    df_new_client, \n",
    "    df_new_historical_weather,\n",
    "    df_new_forecast_weather, \n",
    "    df_new_electricity_prices, \n",
    "    df_new_gas_prices, \n",
    "    df_sample_prediction\n",
    ") in iter_test:\n",
    "\n",
    "    data_storage.update_with_new_data(\n",
    "        df_new_client=df_new_client,\n",
    "        df_new_gas_prices=df_new_gas_prices,\n",
    "        df_new_electricity_prices=df_new_electricity_prices,\n",
    "        df_new_forecast_weather=df_new_forecast_weather,\n",
    "        df_new_historical_weather=df_new_historical_weather,\n",
    "        df_new_target=df_new_target\n",
    "    )\n",
    "    \n",
    "    #separately generate test features for both models\n",
    "    \n",
    "    df_test = data_storage.preprocess_test(df_test)\n",
    "    \n",
    "    df_test_features = features_generator.generate_features(df_test)\n",
    "    \n",
    "    df_test_feats = feat_gen.generate_features(df_test,False)\n",
    "    \n",
    "    df_test_feats.drop(columns=['date','literal'],inplace=True)\n",
    "        \n",
    "    pred1 = predict(df_test_features)\n",
    "    \n",
    "    pred2 = predict_model(df_test_feats)\n",
    "    \n",
    "    # Ensembling with slightly tuned model weights\n",
    "    df_sample_prediction[\"target\"] = (\n",
    "        (0.49 * pred1) + \n",
    "        (0.51 * pred2)\n",
    "    )\n",
    "    \n",
    "    env.predict(df_sample_prediction)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ends"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7292407,
     "sourceId": 57236,
     "sourceType": "competition"
    },
    {
     "datasetId": 4266997,
     "sourceId": 7348254,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4307493,
     "sourceId": 7406639,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
