{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"},{"sourceId":7348254,"sourceType":"datasetVersion","datasetId":4266997},{"sourceId":7406639,"sourceType":"datasetVersion","datasetId":4307493}],"dockerImageVersionId":30627,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transcribed into Korean, Thank you all\n\nÏù¥ ÎÖ∏Ìä∏Î∂ÅÏùÄ \"[Enefit PEBOP: EDA (Plotly) and Modelling](https://www.kaggle.com/code/siddhvr/enefit-pebop-eda-plotly-and-modelling?scriptVersionId=158742203)\" ÎÖ∏Ìä∏Î∂ÅÏùò ÏïΩÍ∞Ñ Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ, <br>\nÏ¶â Í≥µÍ∞ú LBÏóêÏÑú Í∞ÄÏû• ÎÜíÏùÄ Ï†êÏàòÎ•º Î∞õÏùÄ 3Î≤àÏß∏ Î≤ÑÏ†ÑÏûÖÎãàÎã§. \n\nÎ≥ÄÍ≤Ω ÏÇ¨Ìï≠ Î™©Î°ù:\n- ÌïôÏäµÎêú Î™®Îç∏ÏùÄ Ïù¥Ï†ú ÎÖ∏Ìä∏Î∂ÅÏù¥ ÏïÑÎãå Ïô∏Î∂Ä  [dataset](https://www.kaggle.com/datasets/kononenko/v2-enefit-pebop-eda-plotly-and-modelling/data)ÏóêÏÑú Î°úÎìúÎê©ÎãàÎã§. Ïù¥Î°úÏç® ÏÉà ÎÖ∏Ìä∏Î∂Å Î≤ÑÏ†ÑÏù¥ ÏÉùÏÑ±Îê† Í≤ΩÏö∞Ïùò Î¨∏Ï†úÍ∞Ä Ìï¥Í≤∞ÎêòÏóàÏäµÎãàÎã§;\n- ÏïôÏÉÅÎ∏î Í∞ÄÏ§ëÏπòÍ∞Ä LB Ï†êÏàòÎ•º ÏµúÎåÄÌôîÌïòÎèÑÎ°ù Ï°∞Ï†ïÎêòÏóàÏäµÎãàÎã§. \n\nÏù¥ ÎÖ∏Ìä∏Î∂ÅÏù¥ Ïú†Ïö©ÌïòÎã§Í≥† ÏÉùÍ∞ÅÎêòÏãúÎ©¥ @siddhvrÏùò ÏõêÎ≥∏ ÏûëÏóÖÏùÑ Ï∞∏Í≥†Ìï¥ Ï£ºÏÑ∏Ïöî.","metadata":{}},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport gc\nimport pickle\nimport datetime\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport plotly.express as px\nimport joblib\nfrom sklearn.ensemble import VotingRegressor\nimport lightgbm as lgb\nfrom joblib import load\n\nimport holidays","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:11:48.101303Z","iopub.execute_input":"2024-01-17T07:11:48.101966Z","iopub.status.idle":"2024-01-17T07:11:53.717851Z","shell.execute_reply.started":"2024-01-17T07:11:48.10193Z","shell.execute_reply":"2024-01-17T07:11:53.716907Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ïù¥Î≤à Ï£º GPU Ìï†ÎãπÎüâÏùÑ Î™®Îëê ÏÜåÏßÑÌñàÍ∏∞ ÎïåÎ¨∏Ïóê Ïù¥ ÎÖ∏Ìä∏Î∂ÅÏùò Î≥¥ÎÑàÏä§ Î≤ÑÏ†ÑÏûÖÎãàÎã§ üôÉ. \nÏó¨Í∏∞ÏÑúÎäî ÌïôÏäµÎêú Îëê Î™®Îç∏Ïùò ÏòàÏ∏°ÏùÑ ÏïôÏÉÅÎ∏îÌï¥ Î≥¥ÏïòÏäµÎãàÎã§. ÌïòÏßÄÎßå Î¨∏Ï†úÎäî Îëê Î™®Îç∏Ïù¥ ÏÑúÎ°ú Îã§Î•∏ ÏàòÏùò ÌäπÏßïÏùÑ Í∞ÄÏßÑ Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Î°ú ÌïôÏäµÎêòÏóàÎã§Îäî Í≤ÉÏûÖÎãàÎã§. Ïù¥ Ï∞®Ïù¥Í∞Ä ÏµúÏ¢Ö ÏòàÏ∏°Ïóê ÎØ∏ÏπòÎäî ÏòÅÌñ•ÏùÑ ÏÇ¥Ìé¥Î≥¥Î†§Í≥† ÎÖ∏Î†•ÌñàÏäµÎãàÎã§. ","metadata":{}},{"cell_type":"markdown","source":"ÏïûÏÑú Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Î°ú Í≤åÏãúÌïú ÌõàÎ†®Îêú Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÍ≤†ÏäµÎãàÎã§. Ïù¥ Î™®Îç∏ÏùÄ Í≥µÍ∞ú ÌÖåÏä§Ìä∏ ÏÑ∏Ìä∏(Î¶¨ÎçîÎ≥¥Îìú)ÏóêÏÑú 65.51Ïùò ÎèÖÎ¶ΩÏ†ÅÏù∏ MAEÎ•º Í∏∞Î°ùÌñàÏúºÎ©∞, Îã§Î•∏ Î™®Îç∏ÏùÄ Ïù¥ ÎÖ∏Ìä∏Î∂Å [see here](https://www.kaggle.com/code/siddhvr/enefit-pebop-eda-plotly-and-modelling) Ïùò Î≤ÑÏ†Ñ Ï∞∏Ï°∞Î•º ÌÜµÌï¥ ÌïôÏäµÌïú Î™®Îç∏Ïù¥ Îê† Í≤ÉÏûÖÎãàÎã§.","metadata":{}},{"cell_type":"markdown","source":"### Step 1 : Î™®Îì† Îç∞Ïù¥ÌÑ∞ ÌååÏùºÏóê Ïï°ÏÑ∏Ïä§ÌïòÍ≥† Ï≤òÎ¶¨ÌïòÎäî ÌÅ¥ÎûòÏä§Î•º ÎßåÎì≠ÎãàÎã§.","metadata":{}},{"cell_type":"code","source":"class DataStorage:\n    root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n\n    data_cols = [\n        \"target\",\n        \"county\",\n        \"is_business\",\n        \"product_type\",\n        \"is_consumption\",\n        \"datetime\",\n        \"row_id\",\n    ]\n    client_cols = [\n        \"product_type\",\n        \"county\",\n        \"eic_count\",\n        \"installed_capacity\",\n        \"is_business\",\n        \"date\",\n    ]\n    gas_prices_cols = [\"forecast_date\", \"lowest_price_per_mwh\", \"highest_price_per_mwh\"]\n    electricity_prices_cols = [\"forecast_date\", \"euros_per_mwh\"]\n    forecast_weather_cols = [\n        \"latitude\",\n        \"longitude\",\n        \"hours_ahead\",\n        \"temperature\",\n        \"dewpoint\",\n        \"cloudcover_high\",\n        \"cloudcover_low\",\n        \"cloudcover_mid\",\n        \"cloudcover_total\",\n        \"10_metre_u_wind_component\",\n        \"10_metre_v_wind_component\",\n        \"forecast_datetime\",\n        \"direct_solar_radiation\",\n        \"surface_solar_radiation_downwards\",\n        \"snowfall\",\n        \"total_precipitation\",\n    ]\n    historical_weather_cols = [\n        \"datetime\",\n        \"temperature\",\n        \"dewpoint\",\n        \"rain\",\n        \"snowfall\",\n        \"surface_pressure\",\n        \"cloudcover_total\",\n        \"cloudcover_low\",\n        \"cloudcover_mid\",\n        \"cloudcover_high\",\n        \"windspeed_10m\",\n        \"winddirection_10m\",\n        \"shortwave_radiation\",\n        \"direct_solar_radiation\",\n        \"diffuse_radiation\",\n        \"latitude\",\n        \"longitude\",\n    ]\n    location_cols = [\"longitude\", \"latitude\", \"county\"]\n    target_cols = [\n        \"target\",\n        \"county\",\n        \"is_business\",\n        \"product_type\",\n        \"is_consumption\",\n        \"datetime\",\n    ]\n\n    def __init__(self):\n        self.df_data = pl.read_csv(\n            os.path.join(self.root, \"train.csv\"),\n            columns=self.data_cols,\n            try_parse_dates=True,\n        )\n        self.df_client = pl.read_csv(\n            os.path.join(self.root, \"client.csv\"),\n            columns=self.client_cols,\n            try_parse_dates=True,\n        )\n        self.df_gas_prices = pl.read_csv(\n            os.path.join(self.root, \"gas_prices.csv\"),\n            columns=self.gas_prices_cols,\n            try_parse_dates=True,\n        )\n        self.df_electricity_prices = pl.read_csv(\n            os.path.join(self.root, \"electricity_prices.csv\"),\n            columns=self.electricity_prices_cols,\n            try_parse_dates=True,\n        )\n        self.df_forecast_weather = pl.read_csv(\n            os.path.join(self.root, \"forecast_weather.csv\"),\n            columns=self.forecast_weather_cols,\n            try_parse_dates=True,\n        )\n        self.df_historical_weather = pl.read_csv(\n            os.path.join(self.root, \"historical_weather.csv\"),\n            columns=self.historical_weather_cols,\n            try_parse_dates=True,\n        )\n        self.df_weather_station_to_county_mapping = pl.read_csv(\n            os.path.join(self.root, \"weather_station_to_county_mapping.csv\"),\n            columns=self.location_cols,\n            try_parse_dates=True,\n        )\n        self.df_data = self.df_data.filter(\n            pl.col(\"datetime\") >= pd.to_datetime(\"2022-01-01\")\n        )\n        self.df_target = self.df_data.select(self.target_cols)\n\n        self.schema_data = self.df_data.schema\n        self.schema_client = self.df_client.schema\n        self.schema_gas_prices = self.df_gas_prices.schema\n        self.schema_electricity_prices = self.df_electricity_prices.schema\n        self.schema_forecast_weather = self.df_forecast_weather.schema\n        self.schema_historical_weather = self.df_historical_weather.schema\n        self.schema_target = self.df_target.schema\n\n        self.df_weather_station_to_county_mapping = (\n            self.df_weather_station_to_county_mapping.with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n        )\n\n    def update_with_new_data(\n        self,\n        df_new_client,\n        df_new_gas_prices,\n        df_new_electricity_prices,\n        df_new_forecast_weather,\n        df_new_historical_weather,\n        df_new_target,\n    ):\n        df_new_client = pl.from_pandas(\n            df_new_client[self.client_cols], schema_overrides=self.schema_client\n        )\n        df_new_gas_prices = pl.from_pandas(\n            df_new_gas_prices[self.gas_prices_cols],\n            schema_overrides=self.schema_gas_prices,\n        )\n        df_new_electricity_prices = pl.from_pandas(\n            df_new_electricity_prices[self.electricity_prices_cols],\n            schema_overrides=self.schema_electricity_prices,\n        )\n        df_new_forecast_weather = pl.from_pandas(\n            df_new_forecast_weather[self.forecast_weather_cols],\n            schema_overrides=self.schema_forecast_weather,\n        )\n        df_new_historical_weather = pl.from_pandas(\n            df_new_historical_weather[self.historical_weather_cols],\n            schema_overrides=self.schema_historical_weather,\n        )\n        df_new_target = pl.from_pandas(\n            df_new_target[self.target_cols], schema_overrides=self.schema_target\n        )\n\n        self.df_client = pl.concat([self.df_client, df_new_client]).unique(\n            [\"date\", \"county\", \"is_business\", \"product_type\"]\n        )\n        self.df_gas_prices = pl.concat([self.df_gas_prices, df_new_gas_prices]).unique(\n            [\"forecast_date\"]\n        )\n        self.df_electricity_prices = pl.concat(\n            [self.df_electricity_prices, df_new_electricity_prices]\n        ).unique([\"forecast_date\"])\n        self.df_forecast_weather = pl.concat(\n            [self.df_forecast_weather, df_new_forecast_weather]\n        ).unique([\"forecast_datetime\", \"latitude\", \"longitude\", \"hours_ahead\"])\n        self.df_historical_weather = pl.concat(\n            [self.df_historical_weather, df_new_historical_weather]\n        ).unique([\"datetime\", \"latitude\", \"longitude\"])\n        self.df_target = pl.concat([self.df_target, df_new_target]).unique(\n            [\"datetime\", \"county\", \"is_business\", \"product_type\", \"is_consumption\"]\n        )\n\n    def preprocess_test(self, df_test):\n        df_test = df_test.rename(columns={\"prediction_datetime\": \"datetime\"})\n        df_test = pl.from_pandas(\n            df_test[self.data_cols[1:]], schema_overrides=self.schema_data\n        )\n        return df_test\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:11:56.120531Z","iopub.execute_input":"2024-01-17T07:11:56.1209Z","iopub.status.idle":"2024-01-17T07:11:56.146186Z","shell.execute_reply.started":"2024-01-17T07:11:56.120869Z","shell.execute_reply":"2024-01-17T07:11:56.145172Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2: ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ ÌÅ¥ÎûòÏä§ ÏÉùÏÑ±","metadata":{}},{"cell_type":"markdown","source":"Ïù¥Ìï¥ÌïòÍ∏∞ ÏâΩÍ≥† Î™ÖÌôïÌïòÍ≤å ÏÑ§Î™ÖÌïòÍ∏∞ ÏúÑÌï¥ Îëê Î™®Îç∏Ïóê ÎåÄÌï¥ ÏÑúÎ°ú Îã§Î•∏ Îëê Í∞ÄÏßÄ Í∏∞Îä• ÏóîÏßÄÎãàÏñ¥ÎßÅ ÌÅ¥ÎûòÏä§Î•º ÏÇ¨Ïö©ÌïòÍ≤†ÏäµÎãàÎã§. Í∑∏Îü¨ÎÇò ÎåÄÎ∂ÄÎ∂ÑÏùò Ìï®ÏàòÎäî Îã§Î•∏ Í≥≥ÏóêÏÑú ÏïΩÍ∞ÑÏùò Î≥ÄÍ≤ΩÎßå ÏûàÏùÑ Îøê ÎèôÏùºÌïòÎØÄÎ°ú Îëê ÌÅ¥ÎûòÏä§Î•º Î≥ëÌï©ÌïòÏó¨ ÌïòÎÇòÏùò ÌÅ¥ÎûòÏä§Î°ú ÎßåÎì§ ÏàòÎèÑ ÏûàÏäµÎãàÎã§. ÌïòÏßÄÎßå Ïù¥ Í≤ΩÏö∞ Îëê Î™®Îç∏ Î™®ÎëêÏóê ÎåÄÌï¥ ÏÑúÎ°ú Îã§Î•∏ Îëê Í∞úÏùò \"generate_features\" Ìï®ÏàòÎ•º ÏÇ¨Ïö©Ìï¥Ïïº ÌïúÎã§Îäî Ï†êÏùÑ ÏûäÏßÄ ÎßàÏÑ∏Ïöî.","metadata":{}},{"cell_type":"code","source":"class FeatureEngineer:\n    def __init__(self, data):\n        self.data = data\n        self.estonian_holidays = list(\n            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n        )\n\n    def _general_features(self, df_features):\n        df_features = (\n            df_features.with_columns(\n                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n                pl.col(\"datetime\").dt.day().alias(\"day\"),pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n                pl.col(\"datetime\").dt.month().alias(\"month\"),pl.col(\"datetime\").dt.year().alias(\"year\"),\n            )\n            .with_columns(pl.concat_str(\"county\",\"is_business\",\"product_type\",\"is_consumption\",separator=\"_\",).alias(\"segment\"),)\n            .with_columns(\n                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),(np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),(np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n            )\n        )\n        return df_features\n\n    def _client_features(self, df_features):\n        df_client = self.data.df_client\n\n        df_features = df_features.join(\n            df_client.with_columns((pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n            ),on=[\"county\", \"is_business\", \"product_type\", \"date\"],how=\"left\",\n        )\n        return df_features\n    \n    def is_country_holiday(self, row):\n        return (\n            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n            in self.estonian_holidays\n        )\n\n    def _holidays_features(self, df_features):\n        df_features = df_features.with_columns(\n            pl.struct([\"year\", \"month\", \"day\"])\n            .apply(self.is_country_holiday)\n            .alias(\"is_country_holiday\")\n        )\n        return df_features\n\n    def _forecast_weather_features(self, df_features):\n        df_forecast_weather = self.data.df_forecast_weather\n        df_weather_station_to_county_mapping = (\n            self.data.df_weather_station_to_county_mapping\n        )\n\n        df_forecast_weather = (\n            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n#             .drop(\"hours_ahead\")\n            .with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),)\n            .join(df_weather_station_to_county_mapping,how=\"left\",on=[\"longitude\", \"latitude\"],).drop(\"longitude\", \"latitude\"))\n\n        df_forecast_weather_date = (df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\"))\n\n        df_forecast_weather_local = (df_forecast_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean())\n\n        for hours_lag in [0, 7 * 24]:\n            df_features = df_features.join(\n                df_forecast_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),on=\"datetime\",how=\"left\",suffix=f\"_forecast_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_forecast_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),on=[\"county\", \"datetime\"],how=\"left\",suffix=f\"_forecast_local_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _historical_weather_features(self, df_features):\n        df_historical_weather = self.data.df_historical_weather\n        df_weather_station_to_county_mapping = (self.data.df_weather_station_to_county_mapping)\n\n        df_historical_weather = (\n            df_historical_weather.with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            ).join(df_weather_station_to_county_mapping,how=\"left\",on=[\"longitude\", \"latitude\"],).drop(\"longitude\", \"latitude\")\n        )\n\n        df_historical_weather_date = (df_historical_weather.group_by(\"datetime\").mean().drop(\"county\"))\n\n        df_historical_weather_local = (\n            df_historical_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean()\n        )\n\n        for hours_lag in [2 * 24, 7 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),on=\"datetime\",how=\"left\",suffix=f\"_historical_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_historical_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),on=[\"county\", \"datetime\"],how=\"left\",suffix=f\"_historical_local_{hours_lag}h\",\n            )\n\n        for hours_lag in [1 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),).filter(pl.col(\"hour\") <= 10).drop(\"hour\"),on=\"datetime\",how=\"left\",\n                suffix=f\"_historical_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _target_features(self, df_features):\n        df_target = self.data.df_target\n\n        df_target_all_type_sum = (df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\"))\n\n        df_target_all_county_type_sum = (df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\", \"county\"))\n        \n        hours_list=[i*24 for i in range(2,15)]\n\n        for hours_lag in hours_list:\n            df_features = df_features.join(\n                df_target.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n                on=[\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",],\n                how=\"left\",\n            )\n\n        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n            df_features = df_features.join(\n                df_target_all_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],how=\"left\",\n            )\n\n            df_features = df_features.join(\n                df_target_all_county_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n                on=[\"is_business\", \"is_consumption\", \"datetime\"],how=\"left\",\n                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n            )\n\n        cols_for_stats = [f\"target_{hours_lag}h\" for hours_lag in hours_list[:4]]\n        \n        df_features = df_features.with_columns(\n            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n            df_features.select(cols_for_stats).transpose().std().transpose().to_series().alias(f\"target_std\"),\n            )\n\n        for target_prefix, lag_nominator, lag_denomonator in [\n            (\"target\", 24 * 7, 24 * 14),(\"target\", 24 * 2, 24 * 9),(\"target\", 24 * 3, 24 * 10),(\"target\", 24 * 2, 24 * 3),\n            (\"target_all_type_sum\", 24 * 2, 24 * 3),(\"target_all_type_sum\", 24 * 7, 24 * 14),\n            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),(\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n        ]:\n            df_features = df_features.with_columns(\n                (pl.col(f\"{target_prefix}_{lag_nominator}h\")/ (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n            )\n\n        return df_features\n\n    def _reduce_memory_usage(self, df_features):\n        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n        return df_features\n\n    def _drop_columns(self, df_features):\n        df_features = df_features.drop(\"datetime\", \"hour\", \"dayofyear\")\n        return df_features\n\n    def _to_pandas(self, df_features, y):\n        cat_cols = [\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"segment\",]\n\n        if y is not None:\n            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n        else:\n            df_features = df_features.to_pandas()\n\n        df_features = df_features.set_index(\"row_id\")\n        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n\n        return df_features\n    \n    # added some new features here\n    def _additional_features(self,df):\n        for col in [\n                    'temperature', \n                    'dewpoint', \n                    '10_metre_u_wind_component', \n                    '10_metre_v_wind_component', \n            ]:\n            for window in [1]:\n                df[f\"{col}_diff_{window}\"] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])[col].diff(window)\n        return df\n    \n    def _log_outliers(self,df):\n        l1=['installed_capacity', 'target_mean', 'target_std']\n        for i in l1:\n            df = df.with_columns([(f\"log_{i}\", pl.when(df[i] != 0).then(np.log(pl.col(i))).otherwise(0))])\n        return df\n        \n\n    def generate_features(self, df_prediction_items,isTrain):\n        if \"target\" in df_prediction_items.columns:\n            df_prediction_items, y = (\n                df_prediction_items.drop(\"target\"),\n                df_prediction_items.select(\"target\"),\n            )\n        else:\n            y = None\n\n        df_features = df_prediction_items.with_columns(\n            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n        )\n\n        for add_features in [\n            self._general_features,self._client_features,self._forecast_weather_features,\n            self._historical_weather_features,self._target_features,self._holidays_features,\n            self._log_outliers,self._reduce_memory_usage,self._drop_columns,]:\n            df_features = add_features(df_features)\n\n        df_features = self._to_pandas(df_features, y)\n        df_features = self._additional_features(df_features)\n\n        return df_features\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:11:57.594523Z","iopub.execute_input":"2024-01-17T07:11:57.594851Z","iopub.status.idle":"2024-01-17T07:11:57.639474Z","shell.execute_reply.started":"2024-01-17T07:11:57.594825Z","shell.execute_reply":"2024-01-17T07:11:57.638669Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeaturesGenerator:\n    def __init__(self, data_storage):\n        self.data_storage = data_storage\n        self.estonian_holidays = list(\n            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n        )\n\n    def _add_general_features(self, df_features):\n        df_features = (\n            df_features.with_columns(\n                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n                pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n                pl.col(\"datetime\").dt.day().alias(\"day\"),\n                pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n                pl.col(\"datetime\").dt.month().alias(\"month\"),\n                pl.col(\"datetime\").dt.year().alias(\"year\"),\n            )\n            .with_columns(\n                pl.concat_str(\n                    \"county\",\n                    \"is_business\",\n                    \"product_type\",\n                    \"is_consumption\",\n                    separator=\"_\",\n                ).alias(\"segment\"),\n            )\n            .with_columns(\n                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n                (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n                (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n            )\n        )\n        return df_features\n\n    def _add_client_features(self, df_features):\n        df_client = self.data_storage.df_client\n\n        df_features = df_features.join(\n            df_client.with_columns(\n                (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n            ),\n            on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n            how=\"left\",\n        )\n        return df_features\n    \n    def is_country_holiday(self, row):\n        return (\n            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n            in self.estonian_holidays\n        )\n\n    def _add_holidays_features(self, df_features):\n        df_features = df_features.with_columns(\n            pl.struct([\"year\", \"month\", \"day\"])\n            .apply(self.is_country_holiday)\n            .alias(\"is_country_holiday\")\n        )\n        return df_features\n\n    def _add_forecast_weather_features(self, df_features):\n        df_forecast_weather = self.data_storage.df_forecast_weather\n        df_weather_station_to_county_mapping = (\n            self.data_storage.df_weather_station_to_county_mapping\n        )\n\n        df_forecast_weather = (\n            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n            .drop(\"hours_ahead\")\n            .with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n            .join(\n                df_weather_station_to_county_mapping,\n                how=\"left\",\n                on=[\"longitude\", \"latitude\"],\n            )\n            .drop(\"longitude\", \"latitude\")\n        )\n\n        df_forecast_weather_date = (\n            df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\")\n        )\n\n        df_forecast_weather_local = (\n            df_forecast_weather.filter(pl.col(\"county\").is_not_null())\n            .group_by(\"county\", \"datetime\")\n            .mean()\n        )\n\n        for hours_lag in [0, 7 * 24]:\n            df_features = df_features.join(\n                df_forecast_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_forecast_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_forecast_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=[\"county\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_forecast_local_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _add_historical_weather_features(self, df_features):\n        df_historical_weather = self.data_storage.df_historical_weather\n        df_weather_station_to_county_mapping = (\n            self.data_storage.df_weather_station_to_county_mapping\n        )\n\n        df_historical_weather = (\n            df_historical_weather.with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n            .join(\n                df_weather_station_to_county_mapping,\n                how=\"left\",\n                on=[\"longitude\", \"latitude\"],\n            )\n            .drop(\"longitude\", \"latitude\")\n        )\n\n        df_historical_weather_date = (\n            df_historical_weather.group_by(\"datetime\").mean().drop(\"county\")\n        )\n\n        df_historical_weather_local = (\n            df_historical_weather.filter(pl.col(\"county\").is_not_null())\n            .group_by(\"county\", \"datetime\")\n            .mean()\n        )\n\n        for hours_lag in [2 * 24, 7 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_historical_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_historical_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=[\"county\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_historical_local_{hours_lag}h\",\n            )\n\n        for hours_lag in [1 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n                )\n                .filter(pl.col(\"hour\") <= 10)\n                .drop(\"hour\"),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_historical_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _add_target_features(self, df_features):\n        df_target = self.data_storage.df_target\n\n        df_target_all_type_sum = (\n            df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"])\n            .sum()\n            .drop(\"product_type\")\n        )\n\n        df_target_all_county_type_sum = (\n            df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"])\n            .sum()\n            .drop(\"product_type\", \"county\")\n        )\n\n        for hours_lag in [2 * 24,3 * 24,4 * 24,5 * 24,6 * 24,7 * 24,8 * 24,9 * 24,10 * 24,11 * 24,12 * 24,13 * 24,14 * 24,]:\n            df_features = df_features.join(\n                df_target.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n                on=[\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",],\n                how=\"left\",\n            )\n\n        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n            df_features = df_features.join(\n                df_target_all_type_sum.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],\n                how=\"left\",\n            )\n\n            df_features = df_features.join(\n                df_target_all_county_type_sum.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n                on=[\"is_business\", \"is_consumption\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n            )\n\n        cols_for_stats = [\n            f\"target_{hours_lag}h\" for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24]\n        ]\n        df_features = df_features.with_columns(\n            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n            df_features.select(cols_for_stats)\n            .transpose()\n            .std()\n            .transpose()\n            .to_series()\n            .alias(f\"target_std\"),\n        )\n\n        for target_prefix, lag_nominator, lag_denomonator in [\n            (\"target\", 24 * 7, 24 * 14),\n            (\"target\", 24 * 2, 24 * 9),\n            (\"target\", 24 * 3, 24 * 10),\n            (\"target\", 24 * 2, 24 * 3),\n            (\"target_all_type_sum\", 24 * 2, 24 * 3),\n            (\"target_all_type_sum\", 24 * 7, 24 * 14),\n            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),\n            (\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n        ]:\n            df_features = df_features.with_columns(\n                (\n                    pl.col(f\"{target_prefix}_{lag_nominator}h\")\n                    / (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n            )\n\n        return df_features\n\n    def _reduce_memory_usage(self, df_features):\n        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n        return df_features\n\n    def _drop_columns(self, df_features):\n        df_features = df_features.drop(\n            \"date\", \"datetime\", \"hour\", \"dayofyear\"\n        )\n        return df_features\n\n    def _to_pandas(self, df_features, y):\n        cat_cols = [\n            \"county\",\n            \"is_business\",\n            \"product_type\",\n            \"is_consumption\",\n            \"segment\",\n        ]\n\n        if y is not None:\n            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n        else:\n            df_features = df_features.to_pandas()\n\n        df_features = df_features.set_index(\"row_id\")\n        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n\n        return df_features\n    \n    def generate_features(self, df_prediction_items):\n        if \"target\" in df_prediction_items.columns:\n            df_prediction_items, y = (\n                df_prediction_items.drop(\"target\"),\n                df_prediction_items.select(\"target\"),\n            )\n        else:\n            y = None\n\n        df_features = df_prediction_items.with_columns(\n            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n        )\n\n        for add_features in [\n            self._add_general_features,\n            self._add_client_features,\n            self._add_forecast_weather_features,\n            self._add_historical_weather_features,\n            self._add_target_features,\n            self._add_holidays_features,\n            self._reduce_memory_usage,\n            self._drop_columns,\n        ]:\n            df_features = add_features(df_features)\n\n        df_features = self._to_pandas(df_features, y)\n\n        return df_features","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:11:58.28802Z","iopub.execute_input":"2024-01-17T07:11:58.288348Z","iopub.status.idle":"2024-01-17T07:11:58.329703Z","shell.execute_reply.started":"2024-01-17T07:11:58.288323Z","shell.execute_reply":"2024-01-17T07:11:58.328852Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ï¥àÍ∏∞Ìôî Initialisation","metadata":{}},{"cell_type":"code","source":"data_storage = DataStorage()\nfeatures_generator = FeaturesGenerator(data_storage=data_storage)\nfeat_gen = FeatureEngineer(data=data_storage)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:11:59.296615Z","iopub.execute_input":"2024-01-17T07:11:59.297414Z","iopub.status.idle":"2024-01-17T07:12:04.775399Z","shell.execute_reply.started":"2024-01-17T07:11:59.297382Z","shell.execute_reply":"2024-01-17T07:12:04.774152Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ÌîºÏ≥ê ÏÉùÏÑ± Feature Generation","metadata":{}},{"cell_type":"markdown","source":"Ìä∏Î†àÏù¥Îãù Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Î•º ÏÉùÏÑ±Ìï† ÌïÑÏöîÎäî ÏóÜÏßÄÎßå, (Ïó¨Í∏∞ÏÑúÎäî Î™®Îç∏ Ìä∏Î†àÏù¥ÎãùÏùÑ ÏàòÌñâÌïòÏßÄ ÏïäÍ∏∞ ÎïåÎ¨∏Ïóê) Ïó¨Í∏∞ÏÑú ÏÇ¨Ïö©ÌïòÎäî Îëê Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ Í∞ÑÏùò Í∏∞Îä• Ï∞®Ïù¥Î•º Î≥¥Ïó¨Ï£ºÍ∏∞ ÏúÑÌï¥ÏÑúÎßå ÏÉùÏÑ±ÌñàÏäµÎãàÎã§.","metadata":{}},{"cell_type":"code","source":"df_train_features = features_generator.generate_features(data_storage.df_data)\ndf_train_features = df_train_features[df_train_features['target'].notnull()]","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:13:21.937335Z","iopub.execute_input":"2024-01-17T07:13:21.937679Z","iopub.status.idle":"2024-01-17T07:13:45.25749Z","shell.execute_reply.started":"2024-01-17T07:13:21.937646Z","shell.execute_reply":"2024-01-17T07:13:45.256639Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = feat_gen.generate_features(data_storage.df_data,True)\ndf_train = df_train[df_train['target'].notnull()]","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:13:45.258599Z","iopub.execute_input":"2024-01-17T07:13:45.258899Z","iopub.status.idle":"2024-01-17T07:14:09.604826Z","shell.execute_reply.started":"2024-01-17T07:13:45.258856Z","shell.execute_reply":"2024-01-17T07:14:09.604051Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_features.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:14:09.606844Z","iopub.execute_input":"2024-01-17T07:14:09.60717Z","iopub.status.idle":"2024-01-17T07:14:09.613804Z","shell.execute_reply.started":"2024-01-17T07:14:09.607134Z","shell.execute_reply":"2024-01-17T07:14:09.61289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:14:09.615137Z","iopub.execute_input":"2024-01-17T07:14:09.615472Z","iopub.status.idle":"2024-01-17T07:14:09.626384Z","shell.execute_reply.started":"2024-01-17T07:14:09.615444Z","shell.execute_reply":"2024-01-17T07:14:09.625509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:14:09.627754Z","iopub.execute_input":"2024-01-17T07:14:09.628192Z","iopub.status.idle":"2024-01-17T07:14:09.758002Z","shell.execute_reply.started":"2024-01-17T07:14:09.628159Z","shell.execute_reply":"2024-01-17T07:14:09.75694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in df_train.columns:\n    if (col not in df_train_features.columns):\n        print(col)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:14:09.759216Z","iopub.execute_input":"2024-01-17T07:14:09.759518Z","iopub.status.idle":"2024-01-17T07:14:09.771998Z","shell.execute_reply.started":"2024-01-17T07:14:09.759485Z","shell.execute_reply":"2024-01-17T07:14:09.771096Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if('date' in df_train_features.columns):\n    df_train_features.drop(columns=['date'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:14:09.773238Z","iopub.execute_input":"2024-01-17T07:14:09.7738Z","iopub.status.idle":"2024-01-17T07:14:09.782854Z","shell.execute_reply.started":"2024-01-17T07:14:09.773768Z","shell.execute_reply":"2024-01-17T07:14:09.78182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑùÏùÑ ÏúÑÌï¥ PlotlyÎ•º ÏÇ¨Ïö©Ìï† Í≤ÉÏûÖÎãàÎã§. <br>\nPlotlyÏóêÎäî ÎßéÏùÄ Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏ÏóêÏÑú Ïù¥ÏÉÅÍ∞íÏù¥ÎÇò Ïù¥ÏÉÅ ÏßïÌõÑÎ•º Í∞êÏßÄÌï† Ïàò ÏûàÎäî **Ìò∏Î≤Ñ ÎèÑÍµ¨ Í∏∞Îä•**Ïù¥ ÏûàÏäµÎãàÎã§. <br>\nÍ≤∞Í≥º ÌîåÎ°ØÏùÄ Îß§Ïö∞ ÏÉÅÌò∏ ÏûëÏö©Ïù¥ Í∞ÄÎä•ÌïòÎ©∞ Îçî Ïã¨Ï∏µÏ†ÅÏù∏ Î∂ÑÏÑùÏùÑ ÏúÑÌï¥ ÌîåÎ°ØÏùò ÌäπÏ†ï ÏòÅÏó≠ÏùÑ ÌôïÎåÄÌïòÍ≥† ÏßëÏ§ëÌï† Ïàò ÏûàÏäµÎãàÎã§. <br>\nÍ∑∏ÎûòÌîÑÎ•º Î¨¥ÌïúÎåÄÎ°ú ÏÇ¨Ïö©Ïûê ÏßÄÏ†ïÌï† Ïàò ÏûàÏñ¥ ÎçîÏö± ÏùòÎØ∏ ÏûàÍ≥† Ïù¥Ìï¥ÌïòÍ∏∞ Ïâ¨Ïö¥ Í∑∏ÎûòÌîÑÎ•º ÎßåÎì§ Ïàò ÏûàÏäµÎãàÎã§.","metadata":{}},{"cell_type":"markdown","source":"## ÏÑ∏Í∑∏Î®ºÌä∏Î≥Ñ ÏóêÎÑàÏßÄ ÏÜåÎπÑÎüâ segment-wise energy consumption","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)\n\nsegment_list = df_train.segment.unique()[:3]\nc=['orangered','blue','gold']\ni=0\n# Filter the dataset for the specific segment\nfor seg in segment_list:\n    consumption_segment = df_train[df_train.segment == seg]\n\n    # Create a line plot using Plotly Express\n    fig = px.line(consumption_segment, x='date', y='target', \n              title=f'Target Over Time for Segment {seg}',\n              labels={'date': 'Date', 'target': 'Target'},\n              template='plotly_dark',line_shape='linear')\n    fig.update_traces(line=dict(color=c[i], width=1.5))\n\n    # Customize the x-axis date format and tick interval\n    fig.update_xaxes(type='date', tickformat='%Y-%m-%d', tickmode='linear', dtick=15)\n    i+=1\n    # Show the plot\n    fig.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:23:45.593537Z","iopub.execute_input":"2024-01-17T07:23:45.594307Z","iopub.status.idle":"2024-01-17T07:23:48.32944Z","shell.execute_reply.started":"2024-01-17T07:23:45.59427Z","shell.execute_reply":"2024-01-17T07:23:48.328455Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FFT Analysis","metadata":{}},{"cell_type":"markdown","source":"Ï≤òÏùåÏóêÎäî <a href=\"https://www.kaggle.com/chaozhuang\">CHAO ZHUANG</a>Ïùò <a href=\"https://www.kaggle.com/code/chaozhuang/enefit-eda-w-fft-ssa-lgbm-voting-regressor\">amazing notebook</a>ÏóêÏÑú Í≥µÎ∂ÄÌñàÏäµÎãàÎã§. <br>\nÍ∑∏Îäî Îß§Ïö∞ Ïã¨Ï∏µÏ†ÅÏúºÎ°ú Î∂ÑÏÑùÌïòÍ≥† Ïù¥Îèô Ï§ëÏóêÎèÑ Î™®Îì† Í≤ÉÏùÑ ÏÑ§Î™ÖÌï¥ Ï£ºÏóàÏäµÎãàÎã§.","metadata":{}},{"cell_type":"code","source":"import plotly.graph_objects as go\n\nsegment_list = df_train.segment.unique()[:10]\nexample_df = df_train[np.isin(df_train.segment, segment_list)]\nsegments = example_df['segment'].unique()\n\n# Define periods in days and calculate corresponding frequencies\nperiods = {'Annual': 365,'Semiannual': 365 / 2,'Quarterly': 365 / 4,'Monthly': 30,'Biweekly': 14,'Weekly': 7,'Semiweekly': 3.5}\nfrequencies_for_periods = {k: 1 / v for k, v in periods.items()}\n\n# Initialize the figure for the spectra using Plotly\nfig = go.Figure()\n\n# Convert the x-axis to a log scale\nfig.update_xaxes(type='log')\n\n# Plot the spectrum for each segment with offset\nfor i, segment in enumerate(segments):\n    segment_data = example_df[example_df['segment'] == segment]['target']\n    fft_values = np.fft.fft(segment_data)\n    frequencies = np.fft.fftfreq(len(fft_values), d=1)\n    magnitudes = np.abs(fft_values)[frequencies > 0]\n    normalized_magnitudes = magnitudes / np.max(magnitudes)\n    positive_freqs = frequencies[frequencies > 0]\n\n    # Filter out frequencies corresponding to periods longer than 'Semiannual'\n    valid_freqs = positive_freqs[positive_freqs > frequencies_for_periods['Semiannual']]\n    valid_magnitudes = normalized_magnitudes[positive_freqs > frequencies_for_periods['Semiannual']]\n\n    # Offset each segment's spectrum for clarity\n    offset_magnitudes = valid_magnitudes + i\n\n    fig.add_trace(go.Scatter(x=valid_freqs, y=offset_magnitudes, mode='lines', name=f'Segment {segment}'))\n\n# Customize the plot layout\nfig.update_layout(\n    title='Frequency Spectra of hourly target for Each Segment',\n    xaxis_title='Frequency',\n    yaxis_title='Normalized Magnitude + Offset',\n    xaxis=dict(tickvals=list(frequencies_for_periods.values()), ticktext=list(frequencies_for_periods.keys())),\n    showlegend=True\n)\n\n# Show the plot\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:30:43.411805Z","iopub.execute_input":"2024-01-17T07:30:43.412712Z","iopub.status.idle":"2024-01-17T07:30:44.679333Z","shell.execute_reply.started":"2024-01-17T07:30:43.412675Z","shell.execute_reply":"2024-01-17T07:30:44.6777Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fft_plots_enefit(name):\n    # Initialize the figure for the spectrum using Plotly\n    fig = go.Figure()\n\n    # Convert the x-axis to a log scale\n    fig.update_xaxes(type='log')\n\n    # Plot the spectrum for the specified segment\n    segment_data = example_df[example_df['segment'] == '0_0_1_1'][name]\n    fft_values = np.fft.fft(segment_data)\n    frequencies = np.fft.fftfreq(len(fft_values), d=1)\n    magnitudes = np.abs(fft_values)[frequencies > 0]\n    positive_freqs = frequencies[frequencies > 0]\n\n    # Filter out frequencies corresponding to periods longer than 'Semiannual'\n    valid_freqs = positive_freqs[positive_freqs > frequencies_for_periods['Semiannual']]\n    valid_magnitudes = magnitudes[positive_freqs > frequencies_for_periods['Semiannual']]\n\n    fig.add_trace(go.Scatter(x=valid_freqs, y=valid_magnitudes, mode='lines', name='0_0_1_1'))\n\n    #  Customize the plot layout\n    fig.update_layout(\n    title=f'{name} frequency spectrum',\n    xaxis_title='Frequency',\n    yaxis_title='Magnitude',\n    xaxis=dict(tickvals=list(frequencies_for_periods.values()), ticktext=list(frequencies_for_periods.keys())),\n    showlegend=True,\n    )\n\n    # Show the plot\n    fig.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:30:44.681195Z","iopub.execute_input":"2024-01-17T07:30:44.681677Z","iopub.status.idle":"2024-01-17T07:30:44.692847Z","shell.execute_reply.started":"2024-01-17T07:30:44.681631Z","shell.execute_reply":"2024-01-17T07:30:44.692079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_list=['temperature','direct_solar_radiation']\nfor i in plot_list:\n    fft_plots_enefit(i)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:30:45.084577Z","iopub.execute_input":"2024-01-17T07:30:45.08493Z","iopub.status.idle":"2024-01-17T07:30:45.172689Z","shell.execute_reply.started":"2024-01-17T07:30:45.084891Z","shell.execute_reply":"2024-01-17T07:30:45.171866Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Trained Models","metadata":{}},{"cell_type":"code","source":"c1 = load('/kaggle/input/enefit-trained-model/voting_regressor_consumption_model.joblib')\np1 = load('/kaggle/input/enefit-trained-model/voting_regressor_production_model.joblib')\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:35:03.746875Z","iopub.execute_input":"2024-01-17T07:35:03.747897Z","iopub.status.idle":"2024-01-17T07:35:46.164811Z","shell.execute_reply.started":"2024-01-17T07:35:03.747843Z","shell.execute_reply":"2024-01-17T07:35:46.164052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dc1 = load('/kaggle/input/v2-enefit-pebop-eda-plotly-and-modelling/voting_regressor_consumption_model.joblib')\ndp1 = load('/kaggle/input/v2-enefit-pebop-eda-plotly-and-modelling/voting_regressor_production_model.joblib')\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:35:46.16631Z","iopub.execute_input":"2024-01-17T07:35:46.166629Z","iopub.status.idle":"2024-01-17T07:36:33.989245Z","shell.execute_reply.started":"2024-01-17T07:35:46.166601Z","shell.execute_reply":"2024-01-17T07:36:33.98833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Îëê Î™®Îç∏Ïóê ÎåÄÌï¥ Î≥ÑÎèÑÏùò ÏòàÏ∏° Ìï®Ïàò ÏÑ†Ïñ∏ÌïòÍ∏∞","metadata":{}},{"cell_type":"code","source":"def predict(df_features,model_consumption=c1,model_production=p1):\n    predictions = np.zeros(len(df_features))\n\n    mask = df_features[\"is_consumption\"] == 1\n    predictions[mask.values] = model_consumption.predict(\n            df_features[mask]\n    ).clip(0)\n\n    mask = df_features[\"is_consumption\"] == 0\n    predictions[mask.values] = model_production.predict(\n            df_features[mask]\n    ).clip(0)\n\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:36:33.990243Z","iopub.execute_input":"2024-01-17T07:36:33.990486Z","iopub.status.idle":"2024-01-17T07:36:33.996182Z","shell.execute_reply.started":"2024-01-17T07:36:33.990464Z","shell.execute_reply":"2024-01-17T07:36:33.995258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_model(df_features,hours_lag=48,model_consumption=dc1,model_production=dp1):\n    predictions = np.zeros(len(df_features))\n\n    mask = df_features[\"is_consumption\"] == 1\n    predictions[mask.values] = np.clip(\n        df_features[mask][f\"target_{hours_lag}h\"].fillna(0).values+ \n        model_consumption.predict(df_features[mask]),0,np.inf,\n        )\n\n    mask = df_features[\"is_consumption\"] == 0\n    predictions[mask.values] = np.clip(\n        df_features[mask][f\"target_{hours_lag}h\"].fillna(0).values+ \n        model_production.predict(df_features[mask]),0,np.inf,\n        )\n\n    return predictions\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:36:33.997827Z","iopub.execute_input":"2024-01-17T07:36:33.998168Z","iopub.status.idle":"2024-01-17T07:36:34.019417Z","shell.execute_reply.started":"2024-01-17T07:36:33.998145Z","shell.execute_reply":"2024-01-17T07:36:34.018531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit API","metadata":{}},{"cell_type":"code","source":"import enefit\n\nenv = enefit.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:38:38.934152Z","iopub.execute_input":"2024-01-17T07:38:38.935081Z","iopub.status.idle":"2024-01-17T07:38:38.943435Z","shell.execute_reply.started":"2024-01-17T07:38:38.935047Z","shell.execute_reply":"2024-01-17T07:38:38.942651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (\n    df_test, \n    df_new_target, \n    df_new_client, \n    df_new_historical_weather,\n    df_new_forecast_weather, \n    df_new_electricity_prices, \n    df_new_gas_prices, \n    df_sample_prediction\n) in iter_test:\n\n    data_storage.update_with_new_data(\n        df_new_client=df_new_client,\n        df_new_gas_prices=df_new_gas_prices,\n        df_new_electricity_prices=df_new_electricity_prices,\n        df_new_forecast_weather=df_new_forecast_weather,\n        df_new_historical_weather=df_new_historical_weather,\n        df_new_target=df_new_target\n    )\n    \n    #separately generate test features for both models\n    \n    df_test = data_storage.preprocess_test(df_test)\n    \n    df_test_features = features_generator.generate_features(df_test)\n    \n    df_test_feats = feat_gen.generate_features(df_test,False)\n    \n    df_test_feats.drop(columns=['date','literal'],inplace=True)\n        \n    pred1 = predict(df_test_features)\n    \n    pred2 = predict_model(df_test_feats)\n    \n    # Ensembling with slightly tuned model weights\n    df_sample_prediction[\"target\"] = (\n        (0.49 * pred1) + \n        (0.51 * pred2)\n    )\n    \n    env.predict(df_sample_prediction)\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:38:39.470629Z","iopub.execute_input":"2024-01-17T07:38:39.471463Z","iopub.status.idle":"2024-01-17T07:40:35.931735Z","shell.execute_reply.started":"2024-01-17T07:38:39.471433Z","shell.execute_reply":"2024-01-17T07:40:35.930711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ends","metadata":{}}]}