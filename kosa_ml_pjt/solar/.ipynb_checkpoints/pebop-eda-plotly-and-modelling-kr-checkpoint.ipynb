{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"},{"sourceId":7348254,"sourceType":"datasetVersion","datasetId":4266997},{"sourceId":7406639,"sourceType":"datasetVersion","datasetId":4307493}],"dockerImageVersionId":30627,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transcribed into Korean, Thank you all\n\nì´ ë…¸íŠ¸ë¶ì€ \"[Enefit PEBOP: EDA (Plotly) and Modelling](https://www.kaggle.com/code/siddhvr/enefit-pebop-eda-plotly-and-modelling?scriptVersionId=158742203)\" ë…¸íŠ¸ë¶ì˜ ì•½ê°„ ê°œì„ ëœ ë²„ì „, <br>\nì¦‰ ê³µê°œ LBì—ì„œ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì€ 3ë²ˆì§¸ ë²„ì „ì…ë‹ˆë‹¤. \n\në³€ê²½ ì‚¬í•­ ëª©ë¡:\n- í•™ìŠµëœ ëª¨ë¸ì€ ì´ì œ ë…¸íŠ¸ë¶ì´ ì•„ë‹Œ ì™¸ë¶€  [dataset](https://www.kaggle.com/datasets/kononenko/v2-enefit-pebop-eda-plotly-and-modelling/data)ì—ì„œ ë¡œë“œë©ë‹ˆë‹¤. ì´ë¡œì¨ ìƒˆ ë…¸íŠ¸ë¶ ë²„ì „ì´ ìƒì„±ë  ê²½ìš°ì˜ ë¬¸ì œê°€ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤;\n- ì•™ìƒë¸” ê°€ì¤‘ì¹˜ê°€ LB ì ìˆ˜ë¥¼ ìµœëŒ€í™”í•˜ë„ë¡ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤. \n\nì´ ë…¸íŠ¸ë¶ì´ ìœ ìš©í•˜ë‹¤ê³  ìƒê°ë˜ì‹œë©´ @siddhvrì˜ ì›ë³¸ ì‘ì—…ì„ ì°¸ê³ í•´ ì£¼ì„¸ìš”.","metadata":{}},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport gc\nimport pickle\nimport datetime\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport plotly.express as px\nimport joblib\nfrom sklearn.ensemble import VotingRegressor\nimport lightgbm as lgb\nfrom joblib import load\n\nimport holidays","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:11:48.101303Z","iopub.execute_input":"2024-01-17T07:11:48.101966Z","iopub.status.idle":"2024-01-17T07:11:53.717851Z","shell.execute_reply.started":"2024-01-17T07:11:48.10193Z","shell.execute_reply":"2024-01-17T07:11:53.716907Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ì´ë²ˆ ì£¼ GPU í• ë‹¹ëŸ‰ì„ ëª¨ë‘ ì†Œì§„í–ˆê¸° ë•Œë¬¸ì— ì´ ë…¸íŠ¸ë¶ì˜ ë³´ë„ˆìŠ¤ ë²„ì „ì…ë‹ˆë‹¤ ğŸ™ƒ. \nì—¬ê¸°ì„œëŠ” í•™ìŠµëœ ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ì•™ìƒë¸”í•´ ë³´ì•˜ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë¬¸ì œëŠ” ë‘ ëª¨ë¸ì´ ì„œë¡œ ë‹¤ë¥¸ ìˆ˜ì˜ íŠ¹ì§•ì„ ê°€ì§„ ë°ì´í„° ì„¸íŠ¸ë¡œ í•™ìŠµë˜ì—ˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ì°¨ì´ê°€ ìµœì¢… ì˜ˆì¸¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‚´í´ë³´ë ¤ê³  ë…¸ë ¥í–ˆìŠµë‹ˆë‹¤. ","metadata":{}},{"cell_type":"markdown","source":"ì•ì„œ ë°ì´í„° ì„¸íŠ¸ë¡œ ê²Œì‹œí•œ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ê³µê°œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸(ë¦¬ë”ë³´ë“œ)ì—ì„œ 65.51ì˜ ë…ë¦½ì ì¸ MAEë¥¼ ê¸°ë¡í–ˆìœ¼ë©°, ë‹¤ë¥¸ ëª¨ë¸ì€ ì´ ë…¸íŠ¸ë¶ [see here](https://www.kaggle.com/code/siddhvr/enefit-pebop-eda-plotly-and-modelling) ì˜ ë²„ì „ ì°¸ì¡°ë¥¼ í†µí•´ í•™ìŠµí•œ ëª¨ë¸ì´ ë  ê²ƒì…ë‹ˆë‹¤.","metadata":{}},{"cell_type":"markdown","source":"### Step 1 : ëª¨ë“  ë°ì´í„° íŒŒì¼ì— ì•¡ì„¸ìŠ¤í•˜ê³  ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤.","metadata":{}},{"cell_type":"code","source":"class DataStorage:\n    root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n\n    data_cols = [\n        \"target\",\n        \"county\",\n        \"is_business\",\n        \"product_type\",\n        \"is_consumption\",\n        \"datetime\",\n        \"row_id\",\n    ]\n    client_cols = [\n        \"product_type\",\n        \"county\",\n        \"eic_count\",\n        \"installed_capacity\",\n        \"is_business\",\n        \"date\",\n    ]\n    gas_prices_cols = [\"forecast_date\", \"lowest_price_per_mwh\", \"highest_price_per_mwh\"]\n    electricity_prices_cols = [\"forecast_date\", \"euros_per_mwh\"]\n    forecast_weather_cols = [\n        \"latitude\",\n        \"longitude\",\n        \"hours_ahead\",\n        \"temperature\",\n        \"dewpoint\",\n        \"cloudcover_high\",\n        \"cloudcover_low\",\n        \"cloudcover_mid\",\n        \"cloudcover_total\",\n        \"10_metre_u_wind_component\",\n        \"10_metre_v_wind_component\",\n        \"forecast_datetime\",\n        \"direct_solar_radiation\",\n        \"surface_solar_radiation_downwards\",\n        \"snowfall\",\n        \"total_precipitation\",\n    ]\n    historical_weather_cols = [\n        \"datetime\",\n        \"temperature\",\n        \"dewpoint\",\n        \"rain\",\n        \"snowfall\",\n        \"surface_pressure\",\n        \"cloudcover_total\",\n        \"cloudcover_low\",\n        \"cloudcover_mid\",\n        \"cloudcover_high\",\n        \"windspeed_10m\",\n        \"winddirection_10m\",\n        \"shortwave_radiation\",\n        \"direct_solar_radiation\",\n        \"diffuse_radiation\",\n        \"latitude\",\n        \"longitude\",\n    ]\n    location_cols = [\"longitude\", \"latitude\", \"county\"]\n    target_cols = [\n        \"target\",\n        \"county\",\n        \"is_business\",\n        \"product_type\",\n        \"is_consumption\",\n        \"datetime\",\n    ]\n\n    def __init__(self):\n        self.df_data = pl.read_csv(\n            os.path.join(self.root, \"train.csv\"),\n            columns=self.data_cols,\n            try_parse_dates=True,\n        )\n        self.df_client = pl.read_csv(\n            os.path.join(self.root, \"client.csv\"),\n            columns=self.client_cols,\n            try_parse_dates=True,\n        )\n        self.df_gas_prices = pl.read_csv(\n            os.path.join(self.root, \"gas_prices.csv\"),\n            columns=self.gas_prices_cols,\n            try_parse_dates=True,\n        )\n        self.df_electricity_prices = pl.read_csv(\n            os.path.join(self.root, \"electricity_prices.csv\"),\n            columns=self.electricity_prices_cols,\n            try_parse_dates=True,\n        )\n        self.df_forecast_weather = pl.read_csv(\n            os.path.join(self.root, \"forecast_weather.csv\"),\n            columns=self.forecast_weather_cols,\n            try_parse_dates=True,\n        )\n        self.df_historical_weather = pl.read_csv(\n            os.path.join(self.root, \"historical_weather.csv\"),\n            columns=self.historical_weather_cols,\n            try_parse_dates=True,\n        )\n        self.df_weather_station_to_county_mapping = pl.read_csv(\n            os.path.join(self.root, \"weather_station_to_county_mapping.csv\"),\n            columns=self.location_cols,\n            try_parse_dates=True,\n        )\n        self.df_data = self.df_data.filter(\n            pl.col(\"datetime\") >= pd.to_datetime(\"2022-01-01\")\n        )\n        self.df_target = self.df_data.select(self.target_cols)\n\n        self.schema_data = self.df_data.schema\n        self.schema_client = self.df_client.schema\n        self.schema_gas_prices = self.df_gas_prices.schema\n        self.schema_electricity_prices = self.df_electricity_prices.schema\n        self.schema_forecast_weather = self.df_forecast_weather.schema\n        self.schema_historical_weather = self.df_historical_weather.schema\n        self.schema_target = self.df_target.schema\n\n        self.df_weather_station_to_county_mapping = (\n            self.df_weather_station_to_county_mapping.with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n        )\n\n    def update_with_new_data(\n        self,\n        df_new_client,\n        df_new_gas_prices,\n        df_new_electricity_prices,\n        df_new_forecast_weather,\n        df_new_historical_weather,\n        df_new_target,\n    ):\n        df_new_client = pl.from_pandas(\n            df_new_client[self.client_cols], schema_overrides=self.schema_client\n        )\n        df_new_gas_prices = pl.from_pandas(\n            df_new_gas_prices[self.gas_prices_cols],\n            schema_overrides=self.schema_gas_prices,\n        )\n        df_new_electricity_prices = pl.from_pandas(\n            df_new_electricity_prices[self.electricity_prices_cols],\n            schema_overrides=self.schema_electricity_prices,\n        )\n        df_new_forecast_weather = pl.from_pandas(\n            df_new_forecast_weather[self.forecast_weather_cols],\n            schema_overrides=self.schema_forecast_weather,\n        )\n        df_new_historical_weather = pl.from_pandas(\n            df_new_historical_weather[self.historical_weather_cols],\n            schema_overrides=self.schema_historical_weather,\n        )\n        df_new_target = pl.from_pandas(\n            df_new_target[self.target_cols], schema_overrides=self.schema_target\n        )\n\n        self.df_client = pl.concat([self.df_client, df_new_client]).unique(\n            [\"date\", \"county\", \"is_business\", \"product_type\"]\n        )\n        self.df_gas_prices = pl.concat([self.df_gas_prices, df_new_gas_prices]).unique(\n            [\"forecast_date\"]\n        )\n        self.df_electricity_prices = pl.concat(\n            [self.df_electricity_prices, df_new_electricity_prices]\n        ).unique([\"forecast_date\"])\n        self.df_forecast_weather = pl.concat(\n            [self.df_forecast_weather, df_new_forecast_weather]\n        ).unique([\"forecast_datetime\", \"latitude\", \"longitude\", \"hours_ahead\"])\n        self.df_historical_weather = pl.concat(\n            [self.df_historical_weather, df_new_historical_weather]\n        ).unique([\"datetime\", \"latitude\", \"longitude\"])\n        self.df_target = pl.concat([self.df_target, df_new_target]).unique(\n            [\"datetime\", \"county\", \"is_business\", \"product_type\", \"is_consumption\"]\n        )\n\n    def preprocess_test(self, df_test):\n        df_test = df_test.rename(columns={\"prediction_datetime\": \"datetime\"})\n        df_test = pl.from_pandas(\n            df_test[self.data_cols[1:]], schema_overrides=self.schema_data\n        )\n        return df_test\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:11:56.120531Z","iopub.execute_input":"2024-01-17T07:11:56.1209Z","iopub.status.idle":"2024-01-17T07:11:56.146186Z","shell.execute_reply.started":"2024-01-17T07:11:56.120869Z","shell.execute_reply":"2024-01-17T07:11:56.145172Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í´ë˜ìŠ¤ ìƒì„±","metadata":{}},{"cell_type":"markdown","source":"ì´í•´í•˜ê¸° ì‰½ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ê¸° ìœ„í•´ ë‘ ëª¨ë¸ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ë‘ ê°€ì§€ ê¸°ëŠ¥ ì—”ì§€ë‹ˆì–´ë§ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ëŒ€ë¶€ë¶„ì˜ í•¨ìˆ˜ëŠ” ë‹¤ë¥¸ ê³³ì—ì„œ ì•½ê°„ì˜ ë³€ê²½ë§Œ ìˆì„ ë¿ ë™ì¼í•˜ë¯€ë¡œ ë‘ í´ë˜ìŠ¤ë¥¼ ë³‘í•©í•˜ì—¬ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë¡œ ë§Œë“¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ê²½ìš° ë‘ ëª¨ë¸ ëª¨ë‘ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ë‘ ê°œì˜ \"generate_features\" í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤ëŠ” ì ì„ ìŠì§€ ë§ˆì„¸ìš”.","metadata":{}},{"cell_type":"code","source":"class FeatureEngineer:\n    def __init__(self, data):\n        self.data = data\n        self.estonian_holidays = list(\n            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n        )\n\n    def _general_features(self, df_features):\n        df_features = (\n            df_features.with_columns(\n                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n                pl.col(\"datetime\").dt.day().alias(\"day\"),pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n                pl.col(\"datetime\").dt.month().alias(\"month\"),pl.col(\"datetime\").dt.year().alias(\"year\"),\n            )\n            .with_columns(pl.concat_str(\"county\",\"is_business\",\"product_type\",\"is_consumption\",separator=\"_\",).alias(\"segment\"),)\n            .with_columns(\n                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),(np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),(np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n            )\n        )\n        return df_features\n\n    def _client_features(self, df_features):\n        df_client = self.data.df_client\n\n        df_features = df_features.join(\n            df_client.with_columns((pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n            ),on=[\"county\", \"is_business\", \"product_type\", \"date\"],how=\"left\",\n        )\n        return df_features\n    \n    def is_country_holiday(self, row):\n        return (\n            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n            in self.estonian_holidays\n        )\n\n    def _holidays_features(self, df_features):\n        df_features = df_features.with_columns(\n            pl.struct([\"year\", \"month\", \"day\"])\n            .apply(self.is_country_holiday)\n            .alias(\"is_country_holiday\")\n        )\n        return df_features\n\n    def _forecast_weather_features(self, df_features):\n        df_forecast_weather = self.data.df_forecast_weather\n        df_weather_station_to_county_mapping = (\n            self.data.df_weather_station_to_county_mapping\n        )\n\n        df_forecast_weather = (\n            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n#             .drop(\"hours_ahead\")\n            .with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),)\n            .join(df_weather_station_to_county_mapping,how=\"left\",on=[\"longitude\", \"latitude\"],).drop(\"longitude\", \"latitude\"))\n\n        df_forecast_weather_date = (df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\"))\n\n        df_forecast_weather_local = (df_forecast_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean())\n\n        for hours_lag in [0, 7 * 24]:\n            df_features = df_features.join(\n                df_forecast_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),on=\"datetime\",how=\"left\",suffix=f\"_forecast_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_forecast_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),on=[\"county\", \"datetime\"],how=\"left\",suffix=f\"_forecast_local_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _historical_weather_features(self, df_features):\n        df_historical_weather = self.data.df_historical_weather\n        df_weather_station_to_county_mapping = (self.data.df_weather_station_to_county_mapping)\n\n        df_historical_weather = (\n            df_historical_weather.with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            ).join(df_weather_station_to_county_mapping,how=\"left\",on=[\"longitude\", \"latitude\"],).drop(\"longitude\", \"latitude\")\n        )\n\n        df_historical_weather_date = (df_historical_weather.group_by(\"datetime\").mean().drop(\"county\"))\n\n        df_historical_weather_local = (\n            df_historical_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean()\n        )\n\n        for hours_lag in [2 * 24, 7 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),on=\"datetime\",how=\"left\",suffix=f\"_historical_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_historical_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),on=[\"county\", \"datetime\"],how=\"left\",suffix=f\"_historical_local_{hours_lag}h\",\n            )\n\n        for hours_lag in [1 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),).filter(pl.col(\"hour\") <= 10).drop(\"hour\"),on=\"datetime\",how=\"left\",\n                suffix=f\"_historical_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _target_features(self, df_features):\n        df_target = self.data.df_target\n\n        df_target_all_type_sum = (df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\"))\n\n        df_target_all_county_type_sum = (df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\", \"county\"))\n        \n        hours_list=[i*24 for i in range(2,15)]\n\n        for hours_lag in hours_list:\n            df_features = df_features.join(\n                df_target.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n                on=[\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",],\n                how=\"left\",\n            )\n\n        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n            df_features = df_features.join(\n                df_target_all_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],how=\"left\",\n            )\n\n            df_features = df_features.join(\n                df_target_all_county_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n                on=[\"is_business\", \"is_consumption\", \"datetime\"],how=\"left\",\n                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n            )\n\n        cols_for_stats = [f\"target_{hours_lag}h\" for hours_lag in hours_list[:4]]\n        \n        df_features = df_features.with_columns(\n            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n            df_features.select(cols_for_stats).transpose().std().transpose().to_series().alias(f\"target_std\"),\n            )\n\n        for target_prefix, lag_nominator, lag_denomonator in [\n            (\"target\", 24 * 7, 24 * 14),(\"target\", 24 * 2, 24 * 9),(\"target\", 24 * 3, 24 * 10),(\"target\", 24 * 2, 24 * 3),\n            (\"target_all_type_sum\", 24 * 2, 24 * 3),(\"target_all_type_sum\", 24 * 7, 24 * 14),\n            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),(\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n        ]:\n            df_features = df_features.with_columns(\n                (pl.col(f\"{target_prefix}_{lag_nominator}h\")/ (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n            )\n\n        return df_features\n\n    def _reduce_memory_usage(self, df_features):\n        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n        return df_features\n\n    def _drop_columns(self, df_features):\n        df_features = df_features.drop(\"datetime\", \"hour\", \"dayofyear\")\n        return df_features\n\n    def _to_pandas(self, df_features, y):\n        cat_cols = [\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"segment\",]\n\n        if y is not None:\n            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n        else:\n            df_features = df_features.to_pandas()\n\n        df_features = df_features.set_index(\"row_id\")\n        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n\n        return df_features\n    \n    # added some new features here\n    def _additional_features(self,df):\n        for col in [\n                    'temperature', \n                    'dewpoint', \n                    '10_metre_u_wind_component', \n                    '10_metre_v_wind_component', \n            ]:\n            for window in [1]:\n                df[f\"{col}_diff_{window}\"] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])[col].diff(window)\n        return df\n    \n    def _log_outliers(self,df):\n        l1=['installed_capacity', 'target_mean', 'target_std']\n        for i in l1:\n            df = df.with_columns([(f\"log_{i}\", pl.when(df[i] != 0).then(np.log(pl.col(i))).otherwise(0))])\n        return df\n        \n\n    def generate_features(self, df_prediction_items,isTrain):\n        if \"target\" in df_prediction_items.columns:\n            df_prediction_items, y = (\n                df_prediction_items.drop(\"target\"),\n                df_prediction_items.select(\"target\"),\n            )\n        else:\n            y = None\n\n        df_features = df_prediction_items.with_columns(\n            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n        )\n\n        for add_features in [\n            self._general_features,self._client_features,self._forecast_weather_features,\n            self._historical_weather_features,self._target_features,self._holidays_features,\n            self._log_outliers,self._reduce_memory_usage,self._drop_columns,]:\n            df_features = add_features(df_features)\n\n        df_features = self._to_pandas(df_features, y)\n        df_features = self._additional_features(df_features)\n\n        return df_features\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:11:57.594523Z","iopub.execute_input":"2024-01-17T07:11:57.594851Z","iopub.status.idle":"2024-01-17T07:11:57.639474Z","shell.execute_reply.started":"2024-01-17T07:11:57.594825Z","shell.execute_reply":"2024-01-17T07:11:57.638669Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeaturesGenerator:\n    def __init__(self, data_storage):\n        self.data_storage = data_storage\n        self.estonian_holidays = list(\n            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n        )\n\n    def _add_general_features(self, df_features):\n        df_features = (\n            df_features.with_columns(\n                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n                pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n                pl.col(\"datetime\").dt.day().alias(\"day\"),\n                pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n                pl.col(\"datetime\").dt.month().alias(\"month\"),\n                pl.col(\"datetime\").dt.year().alias(\"year\"),\n            )\n            .with_columns(\n                pl.concat_str(\n                    \"county\",\n                    \"is_business\",\n                    \"product_type\",\n                    \"is_consumption\",\n                    separator=\"_\",\n                ).alias(\"segment\"),\n            )\n            .with_columns(\n                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n                (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n                (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n            )\n        )\n        return df_features\n\n    def _add_client_features(self, df_features):\n        df_client = self.data_storage.df_client\n\n        df_features = df_features.join(\n            df_client.with_columns(\n                (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n            ),\n            on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n            how=\"left\",\n        )\n        return df_features\n    \n    def is_country_holiday(self, row):\n        return (\n            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n            in self.estonian_holidays\n        )\n\n    def _add_holidays_features(self, df_features):\n        df_features = df_features.with_columns(\n            pl.struct([\"year\", \"month\", \"day\"])\n            .apply(self.is_country_holiday)\n            .alias(\"is_country_holiday\")\n        )\n        return df_features\n\n    def _add_forecast_weather_features(self, df_features):\n        df_forecast_weather = self.data_storage.df_forecast_weather\n        df_weather_station_to_county_mapping = (\n            self.data_storage.df_weather_station_to_county_mapping\n        )\n\n        df_forecast_weather = (\n            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n            .drop(\"hours_ahead\")\n            .with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n            .join(\n                df_weather_station_to_county_mapping,\n                how=\"left\",\n                on=[\"longitude\", \"latitude\"],\n            )\n            .drop(\"longitude\", \"latitude\")\n        )\n\n        df_forecast_weather_date = (\n            df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\")\n        )\n\n        df_forecast_weather_local = (\n            df_forecast_weather.filter(pl.col(\"county\").is_not_null())\n            .group_by(\"county\", \"datetime\")\n            .mean()\n        )\n\n        for hours_lag in [0, 7 * 24]:\n            df_features = df_features.join(\n                df_forecast_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_forecast_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_forecast_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=[\"county\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_forecast_local_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _add_historical_weather_features(self, df_features):\n        df_historical_weather = self.data_storage.df_historical_weather\n        df_weather_station_to_county_mapping = (\n            self.data_storage.df_weather_station_to_county_mapping\n        )\n\n        df_historical_weather = (\n            df_historical_weather.with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n            .join(\n                df_weather_station_to_county_mapping,\n                how=\"left\",\n                on=[\"longitude\", \"latitude\"],\n            )\n            .drop(\"longitude\", \"latitude\")\n        )\n\n        df_historical_weather_date = (\n            df_historical_weather.group_by(\"datetime\").mean().drop(\"county\")\n        )\n\n        df_historical_weather_local = (\n            df_historical_weather.filter(pl.col(\"county\").is_not_null())\n            .group_by(\"county\", \"datetime\")\n            .mean()\n        )\n\n        for hours_lag in [2 * 24, 7 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_historical_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_historical_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=[\"county\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_historical_local_{hours_lag}h\",\n            )\n\n        for hours_lag in [1 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n                )\n                .filter(pl.col(\"hour\") <= 10)\n                .drop(\"hour\"),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_historical_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _add_target_features(self, df_features):\n        df_target = self.data_storage.df_target\n\n        df_target_all_type_sum = (\n            df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"])\n            .sum()\n            .drop(\"product_type\")\n        )\n\n        df_target_all_county_type_sum = (\n            df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"])\n            .sum()\n            .drop(\"product_type\", \"county\")\n        )\n\n        for hours_lag in [2 * 24,3 * 24,4 * 24,5 * 24,6 * 24,7 * 24,8 * 24,9 * 24,10 * 24,11 * 24,12 * 24,13 * 24,14 * 24,]:\n            df_features = df_features.join(\n                df_target.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n                on=[\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",],\n                how=\"left\",\n            )\n\n        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n            df_features = df_features.join(\n                df_target_all_type_sum.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],\n                how=\"left\",\n            )\n\n            df_features = df_features.join(\n                df_target_all_county_type_sum.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n                on=[\"is_business\", \"is_consumption\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n            )\n\n        cols_for_stats = [\n            f\"target_{hours_lag}h\" for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24]\n        ]\n        df_features = df_features.with_columns(\n            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n            df_features.select(cols_for_stats)\n            .transpose()\n            .std()\n            .transpose()\n            .to_series()\n            .alias(f\"target_std\"),\n        )\n\n        for target_prefix, lag_nominator, lag_denomonator in [\n            (\"target\", 24 * 7, 24 * 14),\n            (\"target\", 24 * 2, 24 * 9),\n            (\"target\", 24 * 3, 24 * 10),\n            (\"target\", 24 * 2, 24 * 3),\n            (\"target_all_type_sum\", 24 * 2, 24 * 3),\n            (\"target_all_type_sum\", 24 * 7, 24 * 14),\n            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),\n            (\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n        ]:\n            df_features = df_features.with_columns(\n                (\n                    pl.col(f\"{target_prefix}_{lag_nominator}h\")\n                    / (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n            )\n\n        return df_features\n\n    def _reduce_memory_usage(self, df_features):\n        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n        return df_features\n\n    def _drop_columns(self, df_features):\n        df_features = df_features.drop(\n            \"date\", \"datetime\", \"hour\", \"dayofyear\"\n        )\n        return df_features\n\n    def _to_pandas(self, df_features, y):\n        cat_cols = [\n            \"county\",\n            \"is_business\",\n            \"product_type\",\n            \"is_consumption\",\n            \"segment\",\n        ]\n\n        if y is not None:\n            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n        else:\n            df_features = df_features.to_pandas()\n\n        df_features = df_features.set_index(\"row_id\")\n        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n\n        return df_features\n    \n    def generate_features(self, df_prediction_items):\n        if \"target\" in df_prediction_items.columns:\n            df_prediction_items, y = (\n                df_prediction_items.drop(\"target\"),\n                df_prediction_items.select(\"target\"),\n            )\n        else:\n            y = None\n\n        df_features = df_prediction_items.with_columns(\n            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n        )\n\n        for add_features in [\n            self._add_general_features,\n            self._add_client_features,\n            self._add_forecast_weather_features,\n            self._add_historical_weather_features,\n            self._add_target_features,\n            self._add_holidays_features,\n            self._reduce_memory_usage,\n            self._drop_columns,\n        ]:\n            df_features = add_features(df_features)\n\n        df_features = self._to_pandas(df_features, y)\n\n        return df_features","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:11:58.28802Z","iopub.execute_input":"2024-01-17T07:11:58.288348Z","iopub.status.idle":"2024-01-17T07:11:58.329703Z","shell.execute_reply.started":"2024-01-17T07:11:58.288323Z","shell.execute_reply":"2024-01-17T07:11:58.328852Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ì´ˆê¸°í™” Initialisation","metadata":{}},{"cell_type":"code","source":"data_storage = DataStorage()\nfeatures_generator = FeaturesGenerator(data_storage=data_storage)\nfeat_gen = FeatureEngineer(data=data_storage)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:11:59.296615Z","iopub.execute_input":"2024-01-17T07:11:59.297414Z","iopub.status.idle":"2024-01-17T07:12:04.775399Z","shell.execute_reply.started":"2024-01-17T07:11:59.297382Z","shell.execute_reply":"2024-01-17T07:12:04.774152Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### í”¼ì³ ìƒì„± Feature Generation","metadata":{}},{"cell_type":"markdown","source":"íŠ¸ë ˆì´ë‹ ë°ì´í„° ì„¸íŠ¸ë¥¼ ìƒì„±í•  í•„ìš”ëŠ” ì—†ì§€ë§Œ, (ì—¬ê¸°ì„œëŠ” ëª¨ë¸ íŠ¸ë ˆì´ë‹ì„ ìˆ˜í–‰í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—) ì—¬ê¸°ì„œ ì‚¬ìš©í•˜ëŠ” ë‘ ë°ì´í„° ì„¸íŠ¸ ê°„ì˜ ê¸°ëŠ¥ ì°¨ì´ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ì„œë§Œ ìƒì„±í–ˆìŠµë‹ˆë‹¤.","metadata":{}},{"cell_type":"code","source":"df_train_features = features_generator.generate_features(data_storage.df_data)\ndf_train_features = df_train_features[df_train_features['target'].notnull()]","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:13:21.937335Z","iopub.execute_input":"2024-01-17T07:13:21.937679Z","iopub.status.idle":"2024-01-17T07:13:45.25749Z","shell.execute_reply.started":"2024-01-17T07:13:21.937646Z","shell.execute_reply":"2024-01-17T07:13:45.256639Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = feat_gen.generate_features(data_storage.df_data,True)\ndf_train = df_train[df_train['target'].notnull()]","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:13:45.258599Z","iopub.execute_input":"2024-01-17T07:13:45.258899Z","iopub.status.idle":"2024-01-17T07:14:09.604826Z","shell.execute_reply.started":"2024-01-17T07:13:45.258856Z","shell.execute_reply":"2024-01-17T07:14:09.604051Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_features.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:14:09.606844Z","iopub.execute_input":"2024-01-17T07:14:09.60717Z","iopub.status.idle":"2024-01-17T07:14:09.613804Z","shell.execute_reply.started":"2024-01-17T07:14:09.607134Z","shell.execute_reply":"2024-01-17T07:14:09.61289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:14:09.615137Z","iopub.execute_input":"2024-01-17T07:14:09.615472Z","iopub.status.idle":"2024-01-17T07:14:09.626384Z","shell.execute_reply.started":"2024-01-17T07:14:09.615444Z","shell.execute_reply":"2024-01-17T07:14:09.625509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:14:09.627754Z","iopub.execute_input":"2024-01-17T07:14:09.628192Z","iopub.status.idle":"2024-01-17T07:14:09.758002Z","shell.execute_reply.started":"2024-01-17T07:14:09.628159Z","shell.execute_reply":"2024-01-17T07:14:09.75694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in df_train.columns:\n    if (col not in df_train_features.columns):\n        print(col)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:14:09.759216Z","iopub.execute_input":"2024-01-17T07:14:09.759518Z","iopub.status.idle":"2024-01-17T07:14:09.771998Z","shell.execute_reply.started":"2024-01-17T07:14:09.759485Z","shell.execute_reply":"2024-01-17T07:14:09.771096Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if('date' in df_train_features.columns):\n    df_train_features.drop(columns=['date'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:14:09.773238Z","iopub.execute_input":"2024-01-17T07:14:09.7738Z","iopub.status.idle":"2024-01-17T07:14:09.782854Z","shell.execute_reply.started":"2024-01-17T07:14:09.773768Z","shell.execute_reply":"2024-01-17T07:14:09.78182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"ë°ì´í„° ë¶„ì„ì„ ìœ„í•´ Plotlyë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. <br>\nPlotlyì—ëŠ” ë§ì€ ë°ì´í„° í¬ì¸íŠ¸ì—ì„œ ì´ìƒê°’ì´ë‚˜ ì´ìƒ ì§•í›„ë¥¼ ê°ì§€í•  ìˆ˜ ìˆëŠ” **í˜¸ë²„ ë„êµ¬ ê¸°ëŠ¥**ì´ ìˆìŠµë‹ˆë‹¤. <br>\nê²°ê³¼ í”Œë¡¯ì€ ë§¤ìš° ìƒí˜¸ ì‘ìš©ì´ ê°€ëŠ¥í•˜ë©° ë” ì‹¬ì¸µì ì¸ ë¶„ì„ì„ ìœ„í•´ í”Œë¡¯ì˜ íŠ¹ì • ì˜ì—­ì„ í™•ëŒ€í•˜ê³  ì§‘ì¤‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. <br>\nê·¸ë˜í”„ë¥¼ ë¬´í•œëŒ€ë¡œ ì‚¬ìš©ì ì§€ì •í•  ìˆ˜ ìˆì–´ ë”ìš± ì˜ë¯¸ ìˆê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ê·¸ë˜í”„ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.","metadata":{}},{"cell_type":"markdown","source":"## ì„¸ê·¸ë¨¼íŠ¸ë³„ ì—ë„ˆì§€ ì†Œë¹„ëŸ‰ segment-wise energy consumption","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)\n\nsegment_list = df_train.segment.unique()[:3]\nc=['orangered','blue','gold']\ni=0\n# Filter the dataset for the specific segment\nfor seg in segment_list:\n    consumption_segment = df_train[df_train.segment == seg]\n\n    # Create a line plot using Plotly Express\n    fig = px.line(consumption_segment, x='date', y='target', \n              title=f'Target Over Time for Segment {seg}',\n              labels={'date': 'Date', 'target': 'Target'},\n              template='plotly_dark',line_shape='linear')\n    fig.update_traces(line=dict(color=c[i], width=1.5))\n\n    # Customize the x-axis date format and tick interval\n    fig.update_xaxes(type='date', tickformat='%Y-%m-%d', tickmode='linear', dtick=15)\n    i+=1\n    # Show the plot\n    fig.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:23:45.593537Z","iopub.execute_input":"2024-01-17T07:23:45.594307Z","iopub.status.idle":"2024-01-17T07:23:48.32944Z","shell.execute_reply.started":"2024-01-17T07:23:45.59427Z","shell.execute_reply":"2024-01-17T07:23:48.328455Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FFT Analysis","metadata":{}},{"cell_type":"markdown","source":"ì²˜ìŒì—ëŠ” <a href=\"https://www.kaggle.com/chaozhuang\">CHAO ZHUANG</a>ì˜ <a href=\"https://www.kaggle.com/code/chaozhuang/enefit-eda-w-fft-ssa-lgbm-voting-regressor\">amazing notebook</a>ì—ì„œ ê³µë¶€í–ˆìŠµë‹ˆë‹¤. <br>\nê·¸ëŠ” ë§¤ìš° ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ì´ë™ ì¤‘ì—ë„ ëª¨ë“  ê²ƒì„ ì„¤ëª…í•´ ì£¼ì—ˆìŠµë‹ˆë‹¤.","metadata":{}},{"cell_type":"code","source":"import plotly.graph_objects as go\n\nsegment_list = df_train.segment.unique()[:10]\nexample_df = df_train[np.isin(df_train.segment, segment_list)]\nsegments = example_df['segment'].unique()\n\n# Define periods in days and calculate corresponding frequencies\nperiods = {'Annual': 365,'Semiannual': 365 / 2,'Quarterly': 365 / 4,'Monthly': 30,'Biweekly': 14,'Weekly': 7,'Semiweekly': 3.5}\nfrequencies_for_periods = {k: 1 / v for k, v in periods.items()}\n\n# Initialize the figure for the spectra using Plotly\nfig = go.Figure()\n\n# Convert the x-axis to a log scale\nfig.update_xaxes(type='log')\n\n# Plot the spectrum for each segment with offset\nfor i, segment in enumerate(segments):\n    segment_data = example_df[example_df['segment'] == segment]['target']\n    fft_values = np.fft.fft(segment_data)\n    frequencies = np.fft.fftfreq(len(fft_values), d=1)\n    magnitudes = np.abs(fft_values)[frequencies > 0]\n    normalized_magnitudes = magnitudes / np.max(magnitudes)\n    positive_freqs = frequencies[frequencies > 0]\n\n    # Filter out frequencies corresponding to periods longer than 'Semiannual'\n    valid_freqs = positive_freqs[positive_freqs > frequencies_for_periods['Semiannual']]\n    valid_magnitudes = normalized_magnitudes[positive_freqs > frequencies_for_periods['Semiannual']]\n\n    # Offset each segment's spectrum for clarity\n    offset_magnitudes = valid_magnitudes + i\n\n    fig.add_trace(go.Scatter(x=valid_freqs, y=offset_magnitudes, mode='lines', name=f'Segment {segment}'))\n\n# Customize the plot layout\nfig.update_layout(\n    title='Frequency Spectra of hourly target for Each Segment',\n    xaxis_title='Frequency',\n    yaxis_title='Normalized Magnitude + Offset',\n    xaxis=dict(tickvals=list(frequencies_for_periods.values()), ticktext=list(frequencies_for_periods.keys())),\n    showlegend=True\n)\n\n# Show the plot\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:30:43.411805Z","iopub.execute_input":"2024-01-17T07:30:43.412712Z","iopub.status.idle":"2024-01-17T07:30:44.679333Z","shell.execute_reply.started":"2024-01-17T07:30:43.412675Z","shell.execute_reply":"2024-01-17T07:30:44.6777Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fft_plots_enefit(name):\n    # Initialize the figure for the spectrum using Plotly\n    fig = go.Figure()\n\n    # Convert the x-axis to a log scale\n    fig.update_xaxes(type='log')\n\n    # Plot the spectrum for the specified segment\n    segment_data = example_df[example_df['segment'] == '0_0_1_1'][name]\n    fft_values = np.fft.fft(segment_data)\n    frequencies = np.fft.fftfreq(len(fft_values), d=1)\n    magnitudes = np.abs(fft_values)[frequencies > 0]\n    positive_freqs = frequencies[frequencies > 0]\n\n    # Filter out frequencies corresponding to periods longer than 'Semiannual'\n    valid_freqs = positive_freqs[positive_freqs > frequencies_for_periods['Semiannual']]\n    valid_magnitudes = magnitudes[positive_freqs > frequencies_for_periods['Semiannual']]\n\n    fig.add_trace(go.Scatter(x=valid_freqs, y=valid_magnitudes, mode='lines', name='0_0_1_1'))\n\n    #  Customize the plot layout\n    fig.update_layout(\n    title=f'{name} frequency spectrum',\n    xaxis_title='Frequency',\n    yaxis_title='Magnitude',\n    xaxis=dict(tickvals=list(frequencies_for_periods.values()), ticktext=list(frequencies_for_periods.keys())),\n    showlegend=True,\n    )\n\n    # Show the plot\n    fig.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:30:44.681195Z","iopub.execute_input":"2024-01-17T07:30:44.681677Z","iopub.status.idle":"2024-01-17T07:30:44.692847Z","shell.execute_reply.started":"2024-01-17T07:30:44.681631Z","shell.execute_reply":"2024-01-17T07:30:44.692079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_list=['temperature','direct_solar_radiation']\nfor i in plot_list:\n    fft_plots_enefit(i)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:30:45.084577Z","iopub.execute_input":"2024-01-17T07:30:45.08493Z","iopub.status.idle":"2024-01-17T07:30:45.172689Z","shell.execute_reply.started":"2024-01-17T07:30:45.084891Z","shell.execute_reply":"2024-01-17T07:30:45.171866Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Trained Models","metadata":{}},{"cell_type":"code","source":"c1 = load('/kaggle/input/enefit-trained-model/voting_regressor_consumption_model.joblib')\np1 = load('/kaggle/input/enefit-trained-model/voting_regressor_production_model.joblib')\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:35:03.746875Z","iopub.execute_input":"2024-01-17T07:35:03.747897Z","iopub.status.idle":"2024-01-17T07:35:46.164811Z","shell.execute_reply.started":"2024-01-17T07:35:03.747843Z","shell.execute_reply":"2024-01-17T07:35:46.164052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dc1 = load('/kaggle/input/v2-enefit-pebop-eda-plotly-and-modelling/voting_regressor_consumption_model.joblib')\ndp1 = load('/kaggle/input/v2-enefit-pebop-eda-plotly-and-modelling/voting_regressor_production_model.joblib')\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:35:46.16631Z","iopub.execute_input":"2024-01-17T07:35:46.166629Z","iopub.status.idle":"2024-01-17T07:36:33.989245Z","shell.execute_reply.started":"2024-01-17T07:35:46.166601Z","shell.execute_reply":"2024-01-17T07:36:33.98833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ë‘ ëª¨ë¸ì— ëŒ€í•´ ë³„ë„ì˜ ì˜ˆì¸¡ í•¨ìˆ˜ ì„ ì–¸í•˜ê¸°","metadata":{}},{"cell_type":"code","source":"def predict(df_features,model_consumption=c1,model_production=p1):\n    predictions = np.zeros(len(df_features))\n\n    mask = df_features[\"is_consumption\"] == 1\n    predictions[mask.values] = model_consumption.predict(\n            df_features[mask]\n    ).clip(0)\n\n    mask = df_features[\"is_consumption\"] == 0\n    predictions[mask.values] = model_production.predict(\n            df_features[mask]\n    ).clip(0)\n\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:36:33.990243Z","iopub.execute_input":"2024-01-17T07:36:33.990486Z","iopub.status.idle":"2024-01-17T07:36:33.996182Z","shell.execute_reply.started":"2024-01-17T07:36:33.990464Z","shell.execute_reply":"2024-01-17T07:36:33.995258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_model(df_features,hours_lag=48,model_consumption=dc1,model_production=dp1):\n    predictions = np.zeros(len(df_features))\n\n    mask = df_features[\"is_consumption\"] == 1\n    predictions[mask.values] = np.clip(\n        df_features[mask][f\"target_{hours_lag}h\"].fillna(0).values+ \n        model_consumption.predict(df_features[mask]),0,np.inf,\n        )\n\n    mask = df_features[\"is_consumption\"] == 0\n    predictions[mask.values] = np.clip(\n        df_features[mask][f\"target_{hours_lag}h\"].fillna(0).values+ \n        model_production.predict(df_features[mask]),0,np.inf,\n        )\n\n    return predictions\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:36:33.997827Z","iopub.execute_input":"2024-01-17T07:36:33.998168Z","iopub.status.idle":"2024-01-17T07:36:34.019417Z","shell.execute_reply.started":"2024-01-17T07:36:33.998145Z","shell.execute_reply":"2024-01-17T07:36:34.018531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit API","metadata":{}},{"cell_type":"code","source":"import enefit\n\nenv = enefit.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:38:38.934152Z","iopub.execute_input":"2024-01-17T07:38:38.935081Z","iopub.status.idle":"2024-01-17T07:38:38.943435Z","shell.execute_reply.started":"2024-01-17T07:38:38.935047Z","shell.execute_reply":"2024-01-17T07:38:38.942651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (\n    df_test, \n    df_new_target, \n    df_new_client, \n    df_new_historical_weather,\n    df_new_forecast_weather, \n    df_new_electricity_prices, \n    df_new_gas_prices, \n    df_sample_prediction\n) in iter_test:\n\n    data_storage.update_with_new_data(\n        df_new_client=df_new_client,\n        df_new_gas_prices=df_new_gas_prices,\n        df_new_electricity_prices=df_new_electricity_prices,\n        df_new_forecast_weather=df_new_forecast_weather,\n        df_new_historical_weather=df_new_historical_weather,\n        df_new_target=df_new_target\n    )\n    \n    #separately generate test features for both models\n    \n    df_test = data_storage.preprocess_test(df_test)\n    \n    df_test_features = features_generator.generate_features(df_test)\n    \n    df_test_feats = feat_gen.generate_features(df_test,False)\n    \n    df_test_feats.drop(columns=['date','literal'],inplace=True)\n        \n    pred1 = predict(df_test_features)\n    \n    pred2 = predict_model(df_test_feats)\n    \n    # Ensembling with slightly tuned model weights\n    df_sample_prediction[\"target\"] = (\n        (0.49 * pred1) + \n        (0.51 * pred2)\n    )\n    \n    env.predict(df_sample_prediction)\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T07:38:39.470629Z","iopub.execute_input":"2024-01-17T07:38:39.471463Z","iopub.status.idle":"2024-01-17T07:40:35.931735Z","shell.execute_reply.started":"2024-01-17T07:38:39.471433Z","shell.execute_reply":"2024-01-17T07:40:35.930711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ends","metadata":{}}]}